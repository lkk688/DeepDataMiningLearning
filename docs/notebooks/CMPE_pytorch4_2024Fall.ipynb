{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 4: Neural Network for MNIST Dataset\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View filled on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/lkk688/DeepDataMiningLearning/blob/master/docs/notebooks/CMPE-pytorch4-2024Fall.ipynb)\n",
        "[![Open filled In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lkk688/DeepDataMiningLearning/blob/master/docs/notebooks/CMPE-pytorch4-2024Fall.ipynb)       \n",
        "**Author:** Kaikai Liu"
      ],
      "metadata": {
        "id": "oB4jrJaT-mr3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHBD6HDpFPfB"
      },
      "source": [
        "## Install Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch is already installed in Colab. If you are installing the pytorch in your local machine, follow the tutorials here: from https://pytorch.org/get-started/locally/. You can check our pytorch installation tutorials for Mac/Windows/Linux/HPC here: https://deepdatamininglearning.readthedocs.io/"
      ],
      "metadata": {
        "id": "Be7eF0nZ-dw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click \"Connect\" to connect to a Colab VM instance. In Runtime->Change Runtime type to select CPU/TPU/GPU."
      ],
      "metadata": {
        "id": "2lUmdFDa-zHE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vyDuiPkFTd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ebbb56-7991-4079-95e0-a35f3f499ee9"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the installed version of PyTorch is lower than required, uninstall it and reinstall again by running the following commands:\n",
        "\n",
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata"
      ],
      "metadata": {
        "id": "8TRnM7K8mGZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check PyTorch Environment\n",
        "\n",
        "Pytorch is very similar to the `numpy` package. Let's start with importing PyTorch. The package is called `torch`, based on its original framework [Torch](http://torch.ch/). As a first step, we can check its version:"
      ],
      "metadata": {
        "id": "ogRnp8eU_kFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47lGOtCC_pCH",
        "outputId": "0fb79f73-82a3-4bc2-c1d5-c15b2553823c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi #check GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XCVSzRy_qN2",
        "outputId": "7ff7e56a-3521-454e-dce6-8b14a910195a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 10 20:33:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fSPXNq_xwC",
        "outputId": "c4716d6d-799d-44c5-abb4-8d00397d7495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.3.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4cdmbms_3bm",
        "outputId": "229dd9ea-0122-48ac-ed21-6c682b573739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6778, 0.7837, 0.6664],\n",
            "        [0.6301, 0.9998, 0.7409],\n",
            "        [0.5495, 0.5978, 0.6948],\n",
            "        [0.2877, 0.6407, 0.9320],\n",
            "        [0.7372, 0.6757, 0.0932]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LCTuzlb_5ed",
        "outputId": "fa13a069-65b0-4db7-c5fc-d4ca124c2184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')  # CUDA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps') #Apple GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "n2x-qnCK_OdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Using device:', device)\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(\"Device name: \", torch.cuda.get_device_name(0))\n",
        "    print(\"Device properties:\", torch.cuda.get_device_properties(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LceVUVNYACu5",
        "outputId": "4e72b56b-1429-4e9c-fd68-30fb1bc2aaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name:  Tesla T4\n",
            "Device properties: _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15102MB, multi_processor_count=40)\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEeWwCUAGKj",
        "outputId": "17035988-5e95-4455-ba14-bcce77676d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.version.cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VZPb2lftAIfb",
        "outputId": "fc211b0e-4ac6-48aa-bac8-f6cc5dd79820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'12.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y3VuCzxALrE",
        "outputId": "d3f376de-5f0b-4fe7-aa49-b4d77b407930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.version()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDna707VAOes",
        "outputId": "2673d42e-a28e-4fb1-a924-fe1f2b060273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8906"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $LD_LIBRARY_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybCKHxPzARMn",
        "outputId": "f097d31a-5720-4ba3-96b7-a14cc31328aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib64-nvidia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in every machine learning framework, PyTorch provides functions that are stochastic like generating random numbers. However, a very good practice is to setup your code to be reproducible with the exact same random numbers. This is why we set a seed below."
      ],
      "metadata": {
        "id": "TfITcdKBAVYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42) # Setting the seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsNtDPlbAUZM",
        "outputId": "d15dd192-f4eb-48b3-ba61-80dfbf037f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78f127be8130>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Google Drive Folder for Colab"
      ],
      "metadata": {
        "id": "ImtLfsKAAdiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Colab, add the following to load google drive folder"
      ],
      "metadata": {
        "id": "H26cKIjkmWNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "PoKlV9Y4maYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filename = os.path.join(\"/content/gdrive/My Drive/data\", filename)"
      ],
      "metadata": {
        "id": "WiI7HRwjmc1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the following tutorial regarding the external data for Colab: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd"
      ],
      "metadata": {
        "id": "6PQi_El2m_ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LMGC0lkUtpOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyd6sYY1KO3s"
      },
      "source": [
        "#Neural networks with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-zH1N9VKSfW"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import helper\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1g3F64uKasx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8805a656-d372-4ed8-f09c-f6e8c1e9cbaf"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Dataset"
      ],
      "metadata": {
        "id": "Z-bqCWaAR6La"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfSZKms9KqI2"
      },
      "source": [
        "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iERJRr07KYoE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "752fb34d4f5f49949a002c7f2aab9bea",
            "3488d1d8dde644c2bd05e823c5b97623",
            "ecf7eb3872314e7789921a07451b242a",
            "3634a03c8bc84c7abbac2115c5507bc2",
            "dc24c31c279c4e52a0793b1483262e07",
            "362559810add4572a2bb94fe24677992",
            "1425539fc33e4af9afa361c62a152b18",
            "88ce042dac3047bb889ab95addec3c5b",
            "77108867ccf14191ba84e64917ab2484",
            "017dd81a504f4074aa5935cbc367533b",
            "6040affd59724270b3e439746c3b9887",
            "ed029c1b8bd34966b8aa328f09599930",
            "8c93212633a74906858ec658cca734cc",
            "ea8e10d5788d4f00a7f599090c7aec3b",
            "346537770db4439d8822b5076d18860f",
            "0082f8c9359f4114a6022e393a6d3fa6",
            "bce76872a48f4f3bb48eb8bb6d3f6cdc",
            "555d1da6dbd94ccf98e3f812f7829e99",
            "d199b2b674bb42b8a7e4d90f0c02587a",
            "975d5256e01e4ec89aeab48542e9e2fc",
            "174be500fe5e44398f10b98e2576c39e",
            "9c701865dfc7454683733166a06db113",
            "6dd21088ddb74294a0d6f1b834e24e59",
            "43bcc1160d9c445fb5523f80e0c7f84a",
            "6bcf60e8b39b4ba88a73ec4ee6303f4a",
            "2f9a827c6c03459eae758e6a7f96025d",
            "c038bfeff2224a18b4c85c2c2110d57f",
            "2b4d2ba717f74732abf8c345b9a92de7",
            "7a4d33f75885425f80add82badfef386",
            "47e6c618e120447cbf4aeef0e51c6e52",
            "025543aef58047f9bb86a2cdc9d2bbdf",
            "688e1bcdeb67474aaf3242ee1e62afc5"
          ]
        },
        "outputId": "d1eb21a3-5667-4ce3-abb6-198d97022e35"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('/content/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "#The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch. And shuffle=True tells it to shuffle the dataset every time we start going through the data loader again."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "752fb34d4f5f49949a002c7f2aab9bea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /content/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77108867ccf14191ba84e64917ab2484",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /content/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce76872a48f4f3bb48eb8bb6d3f6cdc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /content/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bcf60e8b39b4ba88a73ec4ee6303f4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /content/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrbf1bhELCJA"
      },
      "source": [
        "I'm just grabbing the first batch so we can check out the data. We can see below that images is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQlVEqNLLHQq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6fa39d7c-f5e8-498b-db78-2c3a3ef1997c"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VldEUbXBLJiC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "bc6ed535-c983-44c5-ccce-9d2396274f08"
      },
      "source": [
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgCd+f8q1R4ae3U0qAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 251,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Torch NN Module"
      ],
      "metadata": {
        "id": "erFfPcUcSEqo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WIuw1FTLVC6"
      },
      "source": [
        "We'll see how to do it using PyTorch's nn module which provides a much more convenient and powerful method for defining network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv8jcavnLdsS"
      },
      "source": [
        "In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples). However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape (64, 1, 28, 28) to a have a shape of (64, 784), 784 is 28 times 28. This is typically called flattening, we flattened the 2D images into 1D vectors.\n",
        "Previously you built a network with one output unit. Here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TVuTdMjLUfO"
      },
      "source": [
        "def activation(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "# Flatten the input images\n",
        "inputs = images.view(images.shape[0], -1)#(64, 1, 28, 28) to (64, 784)\n",
        "\n",
        "# Create parameters\n",
        "w1 = torch.randn(784, 256)\n",
        "b1 = torch.randn(256)\n",
        "\n",
        "w2 = torch.randn(256, 10)\n",
        "b2 = torch.randn(10)\n",
        "\n",
        "h = activation(torch.mm(inputs, w1) + b1)\n",
        "\n",
        "out = torch.mm(h, w2) + b2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wULiOLjOMLwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1da6ad4a-bf28-4ebe-d210-888cfc3deda8"
      },
      "source": [
        "def softmax(x):\n",
        "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
        "\n",
        "probabilities = softmax(out)\n",
        "\n",
        "# Does it have the right shape? Should be (64, 10)\n",
        "print(probabilities.shape)\n",
        "# Does it sum to 1?\n",
        "print(probabilities.sum(dim=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC-NAetkMW1S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed6030d5-ca07-48fe-a804-9c5cad4051cc"
      },
      "source": [
        "print(probabilities.sum(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.2316e+00, 2.2374e+01, 2.3805e-04, 2.0477e-02, 1.5087e-04, 2.1441e+01,\n",
            "        1.8832e-04, 1.9745e+00, 2.5309e+00, 1.1427e+01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8AXSquCMiEi"
      },
      "source": [
        "PyTorch provides a module nn that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCJmS4rJMjw1"
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-nEJvN9Mmnq"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  #inheriting from nn.Module. Combined with super().__init__() this creates a class that tracks the architecture and provides a lot of useful methods and attributes.\n",
        "  #It is mandatory to inherit from nn.Module when you're creating a class for your network. The name of the class itself can be anything.\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        # creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs\n",
        "        self.hidden = nn.Linear(784, 256)\n",
        "        #You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias.\n",
        "\n",
        "        #this creates another linear transformation with 256 inputs and 10 outputs.\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(256, 10)\n",
        "\n",
        "        # Define sigmoid activation and softmax output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        #Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns.\n",
        "\n",
        "    #PyTorch networks created with nn.Module must have a forward method defined.\n",
        "    #It takes in a tensor x and passes it through the operations you defined in the __init__ method.\n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        #Here the input tensor x is passed through each operation a reassigned to x.\n",
        "        x = self.hidden(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spzhKTHJNr5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b265e453-f95e-4a7f-f6dc-5fae0ddfdc62"
      },
      "source": [
        "# Create the network and look at it's text representation\n",
        "model = Network()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.nn.functional module"
      ],
      "metadata": {
        "id": "NtDayQ8qSVT2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp8jZGaVNy_b"
      },
      "source": [
        "You can define the network somewhat more concisely and clearly using the torch.nn.functional module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as F, import torch.nn.functional as F.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QizW2cHxN3EI"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(784, 256)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hidden layer with sigmoid activation\n",
        "        x = F.sigmoid(self.hidden(x))\n",
        "        # Output layer with softmax activation\n",
        "        x = F.softmax(self.output(x), dim=1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJWRPfHOOxX"
      },
      "source": [
        "Another model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa8jW9hhOSQ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bf1fa34f-b7ae-4eb2-eac9-2040523fef2b"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Defining the layers, 128, 64, 10 units each\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Network()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgqogJKKOVGr"
      },
      "source": [
        "It's good practice to name your layers by their type of network, for instance 'fc' to represent a fully-connected layer. As you code your solution, use fc1, fc2, and fc3 as your layer names.\n",
        "ReLU function is used almost exclusively as the activation function for hidden layers.\n",
        "The only requirement is that for a network to approximate a non-linear function, the activation functions must be non-linear.  Here are a few more examples of common activation functions: Tanh (hyperbolic tangent), and ReLU (rectified linear unit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ydhb7UfOjHq"
      },
      "source": [
        "initializing weights and biases: The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with model.fc1.weight for instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xql_nPNoOvbO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7346208e-7113-48e1-bf21-dfaa9ca844aa"
      },
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0053,  0.0095,  0.0104,  ..., -0.0226,  0.0165,  0.0120],\n",
            "        [-0.0142, -0.0229, -0.0197,  ...,  0.0299, -0.0270, -0.0269],\n",
            "        [-0.0248,  0.0253,  0.0324,  ..., -0.0175, -0.0339,  0.0020],\n",
            "        ...,\n",
            "        [ 0.0054,  0.0103,  0.0017,  ..., -0.0147, -0.0111, -0.0241],\n",
            "        [ 0.0340, -0.0104,  0.0198,  ..., -0.0157, -0.0128, -0.0336],\n",
            "        [-0.0086, -0.0058,  0.0136,  ...,  0.0086,  0.0153,  0.0127]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0080, -0.0018,  0.0310,  0.0209, -0.0279,  0.0038,  0.0211, -0.0280,\n",
            "        -0.0354, -0.0346,  0.0077,  0.0270,  0.0150, -0.0330,  0.0125,  0.0229,\n",
            "        -0.0355, -0.0016,  0.0294,  0.0053,  0.0350,  0.0210,  0.0011,  0.0176,\n",
            "         0.0002, -0.0127, -0.0130, -0.0164, -0.0134, -0.0212, -0.0277,  0.0214,\n",
            "         0.0184,  0.0059, -0.0313, -0.0188,  0.0225,  0.0299,  0.0299,  0.0047,\n",
            "        -0.0238, -0.0196,  0.0241,  0.0200, -0.0347, -0.0111,  0.0093, -0.0025,\n",
            "        -0.0106,  0.0144,  0.0206, -0.0061,  0.0274, -0.0055, -0.0128, -0.0354,\n",
            "         0.0009,  0.0235,  0.0074, -0.0114, -0.0300, -0.0162,  0.0109,  0.0064,\n",
            "         0.0137,  0.0116,  0.0147,  0.0229,  0.0002, -0.0051,  0.0008, -0.0148,\n",
            "        -0.0055, -0.0066,  0.0146, -0.0197,  0.0296, -0.0122, -0.0259, -0.0357,\n",
            "        -0.0261,  0.0071, -0.0005, -0.0032, -0.0273,  0.0274,  0.0201,  0.0258,\n",
            "        -0.0009, -0.0258, -0.0201, -0.0246, -0.0325, -0.0046,  0.0031, -0.0323,\n",
            "        -0.0232, -0.0155,  0.0153,  0.0127, -0.0217,  0.0051,  0.0143, -0.0073,\n",
            "         0.0288, -0.0254, -0.0116, -0.0073,  0.0269,  0.0228, -0.0298, -0.0285,\n",
            "         0.0147,  0.0162, -0.0060, -0.0224, -0.0093,  0.0154, -0.0324,  0.0049,\n",
            "         0.0227,  0.0034,  0.0141,  0.0305,  0.0022,  0.0235,  0.0039,  0.0142],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKXig7oAO3kA"
      },
      "source": [
        "For custom initialization, we want to modify these tensors in place. These are actually autograd Variables, so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T9SPxuPO5fp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "bad0ed5b-9e8d-4307-c6fc-aa8268600363"
      },
      "source": [
        "# Set biases to all zeros\n",
        "model.fc1.bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqTFSchKO99H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2b838f55-b693-48b8-abb6-ec6edc7e58f2"
      },
      "source": [
        "# sample from random normal with standard dev = 0.01\n",
        "model.fc1.weight.data.normal_(std=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6965e-02,  3.2594e-03,  7.2188e-03,  ...,  2.4345e-03,\n",
              "          5.3860e-04,  3.9906e-03],\n",
              "        [-4.0252e-03, -9.4899e-03,  1.6548e-02,  ...,  9.1848e-03,\n",
              "          8.1340e-03, -6.1363e-04],\n",
              "        [ 9.9076e-04,  7.7060e-04, -9.5502e-04,  ...,  6.8196e-03,\n",
              "         -5.0805e-03,  1.7135e-02],\n",
              "        ...,\n",
              "        [ 1.4525e-02,  1.1392e-02, -7.0820e-03,  ...,  6.6372e-03,\n",
              "          8.9632e-03, -3.5935e-04],\n",
              "        [-8.3963e-03, -1.1518e-02, -1.4619e-02,  ..., -1.8183e-05,\n",
              "         -9.8635e-03, -2.0011e-03],\n",
              "        [ 1.0964e-02, -3.1923e-03, -6.5580e-03,  ...,  6.6183e-03,\n",
              "         -1.7163e-02,  1.0629e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the forward pass"
      ],
      "metadata": {
        "id": "Y8cbKJgySbMP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88mlqWudPBqt"
      },
      "source": [
        "Now that we have a network, let's see what happens when we pass in an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3kJhacDQQNb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def test_network(net, trainloader):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Create Variables for the inputs and targets\n",
        "    inputs = Variable(images)\n",
        "    targets = Variable(images)\n",
        "\n",
        "    # Clear the gradients from all Variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, then backward pass, then update weights\n",
        "    output = net.forward(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kp_JXePGm8"
      },
      "source": [
        "\n",
        "# Grab some data\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels)\n",
        "images.resize_(64, 1, 784)\n",
        "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
        "\n",
        "# Forward pass through the network\n",
        "img_idx = 0\n",
        "ps = model.forward(images[img_idx,:])\n",
        "\n",
        "img = images[img_idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDWbnAzMP24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "2c7e794e-202a-44c6-88dd-75321efea3f0"
      },
      "source": [
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAHXCAYAAABd89BGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEAUAADCAEAUAADCAEAUAADDA/wda6PdeOk6zFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 424,
              "height": 235
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Sequential"
      ],
      "metadata": {
        "id": "hP3jZ6O8SgwL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg4rifuJQYxH"
      },
      "source": [
        "Using nn.Sequential: PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, nn.Sequential (documentation). Using this to build the equivalent network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSsTTIgyQfuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "6b97f6e6-2bd7-4e67-ea30-e9eb3d91f086"
      },
      "source": [
        "# Hyperparameters for our network\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model)\n",
        "\n",
        "# Forward pass through the network and display output\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(images.shape[0], 1, 784)\n",
        "ps = model.forward(images[0,:])\n",
        "view_classify(images[0].view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): Softmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAHXCAYAAABd89BGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZglVX038O8PEGQdBUUUo+MCQoIR\n4dXgLmqMCVFxS3wV4xYTt7hE8wZ3jBoxmohLEmMUFzSJUYMmwSjuGybqADFEEI2OBlRQlB0Eh/P+\nUdXStN0zU5fbfe/lfj7Pc5+arqpT9bs1Pd33O+fUqWqtBQAAgK2zzaQLAAAAmCVCFAAAwABCFAAA\nwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABCFAAAwABC\nFAAAwABCFAAws6qq9a/1k65lXkzqml+b81bV2/u2R23tcavqcf36T41WMddlQhQAMHFVtVNVPaWq\n/qWqvlNVl1bVJVX1rap6X1UdUVU7TrrOtVJVGxd9uF94baqq86rqs1X17KraadJ1zqs+YB1VVQdO\nuhYmY7tJFwAAzLeqemCSNyfZa9HqS5JclWR9/3pYkldV1WNaa59Y6xon6JIkF/d/3j7J7knu3r9+\nt6oOba2dO6niZsj3knwtyQ8HtLmgb/OdZbY9Lsm9kmxMcuq1rI0ZpCcKAJiYqnpckg+kC1BfS/KY\nJDdqre3SWtstyQ2SPDzJp5LcLMk9J1PpxLymtbZX/9o9yY2SvCJJS/KL6cInW9Bae15rbb/W2hsH\ntDm+b/M7q1kbs0mIAgAmoqrukORN6T6PfCjJHVtr72qtnbewT2vtgtba+1trhyZ5ZJKLJlPtdGit\nnddae2GSt/WrHlxVN5tkTTCPhCgAYFJenmSHJGcneVRr7bLN7dxae0+Sv9iaA1fVtlX161X1N1W1\noarOqaorquq7VXV8Vd1nM2236e95+WR/D9KVVfWDqvrvqjq2qh6wTJtbVdVfV9WZVXVZf0/Xt6vq\nU1X1vKq60dbUPcDfL/rzQYvq+NkEClW1Q1W9oKq+UlUX9etvsKTuQ6vqn6rq+/31+f6Wrs+S9gdU\n1T/07S6vqjOq6kVVtcMK++/aX9t/rKrTqur8/np9o6reXFX7rNJ5V5xYYjPn+LmJJRbWpRvKlyRv\nW3Lf2sZ+v2P7r9+3hXO8tN/vpK2ti+ngnigAYM1V1d5JDuu/fH1r7YKtaddaa1t5iv3T9W4tuDDJ\nFUlumuTwJIdX1fNba69cpu1xSR616OsLkuyWbijdL/avDy9srKqD0g033LVfdWW6e5lu0b/uleSU\nxW3G4OxFf95tme3XT/KZJHfu67l06Q5V9fIkL+i/bOne5565+voc3Vp73mZquGu64YQ7p7u+leR2\nSf4kyW9U1a+21i5e0uaxSd7Q/3lTf85tktymfz2qqg5vrX1szOcdl8uSnJPu3rTr9edfHP5/0C/f\nkuTxSR5YVXss7l1dUFXbpLseSXLsKtXLKtETBQBMwr3TffhNkn9eheNfke6D6a8lWddaW9da2yXJ\nTZK8KN0H+FdU1a8sblRV90wXoDYleXaS3VprN0gXSm6WbkKBzy0512vSBaj/SHJQa2371toN033I\nv1OSY9KFhXG6xaI/n7/M9qcl2TfdEMhd+vewPl24S1U9MlcHqDcm2bOv+ca5OuQcWVVHbKaGv0ry\n1SS/3Fpbl+4aPD5dqDgky/ca/jDdPV13TrJTa22PdNd2/yTvTnfN/q6qdh7zeceitfae1tpeSRZ6\njp656J61vVprd+r3O6mvcfskj17hcPdJcst0fyfvWa2aWR1CFAAwCfv3y5+km1BirFprZ7bWntha\nO7G1duGi9ee21l6e5KXpQtyTlzQ9pF9+tLV2TGvtor5da619r7X2jtbac1do88zW2imLznVpa+3L\nrbVnt9a+MNY3mDypX16V5EvLbN8lyW/3H/qv6Ov5dmvtyqqqJC/r9/uH1toftNZ+2O9zXmvtGbl6\nuODL+h6T5fwkyQNaa//Vt72itfb2JE/ttz+xqhaHvbTW/qG19sLW2pcW1dVaa2ekm1TkY+mC3MM3\n894Hn3dC3tIvH7/C9if0y/ctfJ8xO4QoAGAS9uiXPx4wRG+c/qVf3m3J+oXAtedmwsNSC21ueq2r\n2oyq2r6qfrGq3pJuyvckeU9r7QfL7P6V1tqJKxzqwCS37f/88hX2eWm/XJ+u12g5b2qt/WiZ9e9M\ncla6z5kPXaHtz+m/D07ov1z697Jq511F70zXI3pgVd1x8Yb+3rSH9F8ayjeDhCgA4DqpqnbsH0r7\nqao6t58govUTAyz0GC2d2e7j6T74HpTkU9U95HdLs98t3Hv1zqo6uqoOqarrjeltvGRRzT9J8t9J\nnthv+/dc3fuy1OZ6vhYmovhBa+2/l9uhtfa1XH3f1UHL7ZPuPrDl2l6V5LMrta2qm1fVq/oJP86v\n7iHCC+/xtf1um7vmI513rfX3QX2g/3Jpb9T/TTeM8euttc+saWGMhRAFAEzCwo32N+yHl41VVd00\n3UNQ/yLdxA43ThdCfpBuYoCFh65e496b1trXkzwl3f0190g3ycTZVfWtfva9a/Qo9P4o3T0yuyb5\n43QB5sKq+kRVPaWqdrwWb+WSvt5zknw3yelJ/ind0Ld7tNaWux8quXqCg+XcuF+evZl9kq5XZ/H+\nS22u/cK2a7Stqnulew//L13QWZdu2vqF97jQq7e5e6IGn3eCFob0Paqqtl+0fmEo39vCTBKiAIBJ\nOL1f7pBuZrVxOybdxArfTDf0bff+Ab579hMDHLJSw9basUluleRZST6YLvCtT3f/1Iaqev6S/c9L\ncvckv5rk9el6ubZPcmi6SRBOq6qbj/g+Fj9sd+/W2i+21h7WP0/rp5tpt2krjn39EWsaSd879650\n92t9LN2Dk3dsrd1g4T0m+cOF3deytlX0sSTfSjd89UFJNz17kv+T7u/oHZMrjWtDiAIAJuHT6abV\nTvoPl+PS/4//g/svH91a+6fW2o+X7HaTzR2jtXZOa+11rbXD0/Vq3DnJ8ek+3L+sqn55yf6ttfax\n1tozW2sHpZsO/feT/CjJrXP1MLVpsNBL9Qtb2G8h+K3Uq7W5IXcL2xa3vUt/zB8leXBr7bOttcuX\ntNvs38uI552Y/j6vhXueFob0LfRCfaS19t21r4pxEKIAgDXXWjsrV99L9AdVtdyzjn7OVg79u1G6\nHq7k6nuflrrf1pwv+VlA+lKSR+TqiQvuvoU2P26tvTnJQq/VvTa3/xo7uV/uXFXLThpRVfsm2XvJ\n/kst+576v6N7LtN2IZSd2Vr7uedW9bbm72XoeVfDVQun3Yp935au1+nXquqWSRamjTehxAwTogCA\nSXlhuvuUbp7u2UCbHV5WVb+Vq4d7bc5FubqX6/bLHOemSf5ghXNsv9z6JGmtbUr34NqkD2lVtU1V\nbbeZWi5bvP+UODXJN/o/P3+FfY7qlxuTfHGFfZ7SzzK31BHp/k6vSnf/1oKFZ2Xts9zfdVXdP90Q\nyC0Zet7VsHDv1nJ1XENr7ewk/5Zk23TPwrpxup6y1Xg+GmtEiAIAJqK1dmq6h8K2JIclOaWfDW/3\nhX2qal1VPbSqPpnugaS7bsVxL0o3c12SHFtVB/bH2qaq7ptuKOFKPQh/WlXvq6rDl9Rxk6p6fbp7\npVqSj/abdkvyjap6QVXdvqq2XXKuV/T7fWTLV2Rt9EPMXth/+eCqekNV7ZEkVbVH/z7/b7/9hf2s\nd8u5fpIP9/f4pKquV1WPTfKmfvtbW2vfWbT/55Ncmu7+oHf2YXZhFsUnJHl/rp5wZHOGnnc1LMxq\n+NCqWrcV+y9MMLEwdfu7WmtXrrQz029z/3MCALCqWmtvrarzkvxNkv3SzYaXqro4XVhZHJq+neQT\nW3noZyf5ZLqeqFOq6pJ0/3m8Y7p7cp6Qq6efXmy7dBNRPKyv48J0gWtxHS9srZ226Otbpnve0suT\nXFlVF6WbdW7bfvs3s3U9aGumtfaeqrp9khckeXqSp1bVBenqXvhP9qNba+/ezGGemuRvk/xX33bH\ndBNqJF2IvcZ7bq2dX1XPS/K6dEMjH9G32znddT813RC312+h/EHnXSXHJXluumGdP6yqc9P1Up7V\nWltuqOcJSb6Xq58lZijfjNMTBQBMVGvtA+kmX3hauvukzkr3oXq7dMPJ3pfkUUlut7XP1Gmt/Ue6\niQw+kOTHSa6X5Nx0Ye3AJP+5QtPXJnlGuln5zkwXoHZI8r/pesLu2Vr700X7X5jkN9PNBvjFdMO0\ndk03NfmX0oWUA/t7wKZKa+2FSe6b7r3+MN2seeelG2Z2v9ba87ZwiJOS/EqSf0w3LLMl+VqSFye5\nd2vt4mXO+fp0D8Jd6JXaLskZSV6S5K7phmJuyeDzjltr7Yx0szF+ON0wxb3ShellZ2HsZ1JceMDz\nl5aEcGZQTeYh4QAAMD+q6swk+yR5SmvtTVvan+kmRAEAwCrq74/7WLoeypu11i7cQhOmnOF8AACw\nSqrqRkle3X95rAB13aAnCgAAxqyqXpPkt9LdL3W9dPed/VJr7dyJFsZY6IkCAIDxu1GSX0j3rLAT\nk9xHgLru0BMFAAAwgJ4oAACAAYQoAACAAbYbteGvbvMI4wAB5txHr3pvTboGAFhreqIAAAAGEKIA\nAAAGGHk4HwDMsqr6VpLdkmyccCkATMb6JBe21m41tKEQBcC82m3HHXfcff/999990oUAsPZOP/30\nXHbZZSO1FaIAmFcb999//903bNgw6ToAmICDDz44J5988sZR2ronCgAAYAAhCgAAYAAhCgAAYAAh\nCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAAYAAhCgAA\nYAAhCgAAYAAhCgAAYIDtJl0AAEzKaWdfkPVHnjDpMla08ejDJl0CAMvQEwUAADCAEAUAADCAEAUA\nADCAEAUAADCAEAUAADCAEAUAADCAEAUAADCAEAXA1KrOk6rqP6rq4qq6pKq+XFVPriq/wwCYCL+A\nAJhm70ry5iTrk/x9krck2SnJXyd5+8SqAmCubTfpAgBgOVX1kCSPSvKtJHdurf2wX799kvcneUxV\nfaC19k8TLBOAOaQnCoBp9ZB++ecLASpJWmtXJHlR/+XT17wqAOaeEAXAtNqrX35zmW0L6+7R90wB\nwJoxnA+AabXQ+3SrZbbdul9u1//5jJUOUlUbVti03+ilATDP9EQBMK1O6Jd/WFW7L6ysqusleemi\n/W64plUBMPf0RAEwrf4hyWOS/FqSr1bVB5NcnuR+SW6a5DtJbpHkqs0dpLV28HLr+x6qg8ZZMADz\nQU8UAFOptbYpyQOTHJnkB0ke27++nuSuSS7qdz13IgUCMLf0RAEwtVprVyZ5Vf/6maq6fpJ9kvyw\ntfatSdQGwPzSEwXALHpkku3TPYAXANaUEAXA1Kqq3ZZZd2CSVyf5cZKj17woAOae4XwATLOPVtVl\nSU5Ldw/U/kkOS3JZkge21r47yeIAmE9CFADT7H3phu4dkWTHJGcneXOSV7bWzppkYQDMLyEKgKnV\nWnt1uqF7ADA13BMFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgNn5AJhbB+y9LhuOPmzSZQAw\nY/REAQAADCBEAQAADCBEAQAADCBEAQAADCBEAQAADCBEAQAADGCKcwDm1mlnX5D1R54w6TKuYaMp\n1wGmnp4oAACAAYQoAACAAYQoAACAAYQoAACAAYQoAACAAYQoAACAAYQoAACAAYQoAKZaVR1WVSdW\n1VlVdVlVfbOq3ltVd5l0bQDMJyEKgKlVVa9K8q9JDkry4SSvS3Jykgcn+XxVHTHB8gCYU9tNugAA\nWE5V7ZXkuUnOSfLLrbVzF207NMknkvxJkndNpkIA5pWeKACm1S3T/Z76j8UBKklaa59MclGSG0+i\nMADmmxAFwLT6epIrkty5qm60eENV3TPJrkk+NonCAJhvhvMBMJVaaz+qqj9O8hdJvlpVH0hyXpLb\nJHlQko8m+f0tHaeqNqywab9x1QrAfBGiAJharbVjqmpjkmOTPGnRpm8kefvSYX4AsBYM5wNgalXV\n/0vyviRvT9cDtXOSg5N8M8m7q+rPtnSM1trBy72SnLGKpQNwHSZEATCVqureSV6V5J9ba3/YWvtm\na+3S1trJSR6S5Owkz6mqW0+yTgDmjxAFwLT6zX75yaUbWmuXJvliut9jd1zLogBAiAJgWu3QL1ea\nxnxh/RVrUAsA/IwQBcC0+my//L2q2nvxhqr69SR3S3J5kpPWujAA5pvZ+QCYVu9L9xyo+yU5vaqO\nT/L9JPunG+pXSY5srZ03uRIBmEdCFABTqbV2VVX9RpKnJXlkuskkdkryoyQfSvL61tqJEywRgDkl\nRAEwtVprVyY5pn8BwFRwTxQAAMAAQhQAAMAAQhQAAMAAQhQAAMAAQhQAAMAAQhQAAMAApjgHYG4d\nsPe6bDj6sEmXAcCM0RMFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgCnOAZhbp519\nQdYfecKky/g5G027DjDV9EQBAAAMIEQBAAAMIEQBAAAM4J4oYOIu/807j9Tuyp1H+3+gc0c7Xa7a\n48qR2u383zuM1O5mrz5ppHYAwOrSEwUAADCAEAUAADCAEAXAVKqqx1VV28Jr06TrBGD+uCcKgGl1\napKXrrDtHknuk+Tf1q4cAOgIUQBMpdbaqemC1M+pqi/0f3zz2lUEAB3D+QCYKVV1+ySHJDk7yQkT\nLgeAOSREATBrfq9fvrW15p4oANacEAXAzKiqHZMckWRTkrdMuBwA5pR7ogCYJb+V5AZJTmit/e/W\nNKiqDSts2m9sVQEwV/REATBLFoby/c1EqwBgrumJAmAmVNUvJblrkrOSfGhr27XWDl7heBuSHDSe\n6gCYJ3qiAJgVJpQAYCoIUQBMvaq6fpLHpJtQ4q0TLgeAOWc4H6y1bbYdqdm2t7v1SO1Of+YNR2r3\nykPfO1K7++101uA267ZZ6b7/zTtn02UjtbvgqtH+Dkb1uBs9drSGrx5vHTPuEUlumORft3ZCCQBY\nLXqiAJgFC0P53jzRKgAgQhQAU66q9k9y9wycUAIAVovhfABMtdba6Ulq0nUAwAI9UQAAAAMIUQAA\nAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAOY4hyAuXXA3uuy4ejDJl0GADNGTxQAAMAAQhQAAMAA\nhvPBiK66xx1Hanfli348UruP/9J7Rmo3qo9ftsNI7e520lMGt9n9gzuNdK7dT/ruSO1+uvE7I7Ub\n1e45c03PBwCsLj1RAAAAAwhRAAAAAwhRAAAAA7gnCoC5ddrZF2T9kSdMuowkyUZTrQPMDD1RAAAA\nAwhRAAAAAwhRAAAAAwhRAAAAAwhRAAAAAwhRAAAAAwhRAEy9qrpvVR1fVd+vqp9U1Xer6iNV9RuT\nrg2A+eM5UQBMtar6syR/lOSsJP+c5IdJbpzk4CT3TvKhiRUHwFwSogCYWlX1pHQB6h1Jfq+1dsWS\n7debSGEAzDUhiuuUy3/zziO1O+RlXxzc5nE3/MuRznXOpl1GarfP+58yUrtd/2fbkdrt/a4zRmq3\n/ryvjNRuFD9dszMxCVW1Q5JXJPlOlglQSdJau3LNCwNg7glRAEyrX003bO+YJFdV1WFJDkhyeZIv\ntta+MMniAJhfQhQA0+pO/fLyJKekC1A/U1WfSfLw1toP1rowAOabEAXAtNqzX/5Rkq8muUeSU5Pc\nKslrktw/yXvTTS6xoqrasMKm/cZSJQBzxxTnAEyrhd9RP03yoNba51prF7fW/ivJQ9LN1nevqrrL\nxCoEYC7piQJgWp3fL09prW1cvKG1dmlVfSTJE5PcOcmK90e11g5ebn3fQ3XQeEoFYJ7oiQJgWn2t\nX56/wvYf98sd16AWAPgZIQqAafXxJC3JL1bVcr+vFiaa+NbalQQAQhQAU6q19u0k/5LkFkmeuXhb\nVd0/ya+l66X68NpXB8A8c08UANPsaUnumOQv+udEnZJudr7Dk2xK8ruttQsmWB8Ac0iIAmBqtdbO\nqqqDk7w4yYOS3DPJhel6qF7ZWvviJOsDYD4JUQBMtf5hun/QvwBg4twTBQAAMIAQBQAAMIDhfEyl\nnxx2p5HaPfu1fzdSuwfudOEIra4/0rke+rYnj9Ru3z/98kjt2pVXjNRu00itAACu+/REAQAADCBE\nAQAADCBEAQAADOCeKADm1gF7r8uGow+bdBkAzBg9UQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAA\nAAMIUQAAAAOY4hyAuXXa2Rdk/ZEnTOz8G02vDjCT9EQBAAAMIEQBAAAMYDgfq+qqux84Uru//+vX\njtRuz213GqndWjr99/9qpHbvftQeI7X7s9PvP1K7mz786yO1az/96UjtAABmhZ4oAACAAYQoAACA\nAYQoAACAAYQoAACAAYQoAKZWVW2sqrbC6/uTrg+A+WR2PgCm3QVJjllm/cVrXQgAJEIUANPv/Nba\nUZMuAgAWGM4HAAAwgJ4oAKbdDlV1RJJbJLkkyVeSfKa1tmmyZQEwr4QoAKbdXkmOW7LuW1X1+Nba\np7fUuKo2rLBpv2tdGQBzyXA+AKbZ25LcN12Q2jnJ7ZP8TZL1Sf6tqu4wudIAmFd6ogCYWq21ly5Z\ndVqSJ1fVxUmek+SoJA/ZwjEOXm5930N10BjKBGDO6IkCYBa9qV/ec6JVADCX9ESxVba7+d4jtfuT\n4948Urs9t91ppHaj2uejTxrcZttztx/pXNe7oEZq9+rHHztSu5Pv9O6R2j3gLk8cqd02nz1lpHYw\n0A/65c4TrQKAuaQnCoBZdEi//OZEqwBgLglRAEylqtq/qn6up6mq1id5Y//lu9ayJgBIDOcDYHr9\ndpLnVNVnknw7yUVJbpPksCTXT/KhJK+ZXHkAzCshCoBp9ckkt0tyxyR3S3f/0/lJPpfuuVHHtdba\n5MoDYF4JUQBMpf5Bult8mC4ArDX3RAEAAAwgRAEAAAwgRAEAAAwgRAEAAAwgRAEAAAxgdj4A5tYB\ne6/LhqMPm3QZAMwYPVEAAAAD6Iliq7Qddxip3RFffuJI7X76P7uM1G7fN509WrvvnTa4TfvJT0Y6\n16iOvPIJI7V7wDPfOFK7bS+9YqR2nnwKAFzX6YkCAAAYQIgCAAAYQIgCAAAYQIgCAAAYwMQSAMyt\n086+IOuPPGHSZSRJNppqHWBm6IkCAAAYQIgCAAAYQIgCAAAYQIgCAAAYQIgCAAAYQIgCAAAYQIgC\nAAAYQIgCYGZU1RFV1frX7066HgDmk4ftslU2ff2bI7W7xSPGXMgW/HRtT7emNm0/WruPXrbjSO22\nuejykdptGqkVbFlV/UKSNya5OMkuEy4HgDmmJwqAqVdVleRtSc5L8qYJlwPAnBOiAJgFz0hynySP\nT3LJhGsBYM4JUQBMtaraP8nRSV7XWvvMpOsBAPdEATC1qmq7JMcl+U6S5494jA0rbNpv1LoAmG9C\nFADT7MVJ7pjk7q21yyZdDAAkQhQAU6qqfiVd79Oft9a+MOpxWmsHr3D8DUkOGvW4AMwv90QBMHX6\nYXzvTHJmkhdNuBwAuAYhCoBptEuSfZPsn+TyRQ/YbUle0u/zt/26YyZWJQBzyXA+AKbRT5K8dYVt\nB6W7T+pzSb6WZOShfgAwCiEKgKnTTyLxu8ttq6qj0oWod7TW3rKWdQFAYjgfAADAIEIUAADAAEIU\nADOltXZUa60M5QNgUtwTBWts29veaqR2hz/scyO1e+rnHz1Su33OPHmkdgAA13V6ogAAAAYQogAA\nAAYQogAAAAYQogAAAAYwsQQAc+uAvddlw9GHTboMAGaMnigAAIABhCgAAIABhCgAAIABhCgAAIAB\nhCgAAIABhCgAAIABTHEOwNw67ewLsv7IEyZdxs/ZaNp1gKmmJwoAAGAAPVGwxs67614jtXvZnu8f\nqd3x/3P3kdoBALA8PVEAAAADCFEAAAADCFEAAAADCFEAAAADCFEAAAADCFEATK2qelVVfbyq/req\nLquqH1XVKVX1kqraY9L1ATCfhCgAptmzk+yc5KNJXpfk3Ul+muSoJF+pql+YXGkAzCvPiQJgmu3W\nWrt86cqqekWS5yd5XpKnrnlVAMw1PVEATK3lAlTvH/vlPmtVCwAsEKIAmEUP7JdfmWgVAMwlw/kA\nmHpV9dwkuyRZl+T/JLl7ugB19Fa03bDCpv3GViAAc0WIAmAWPDfJTRZ9/eEkj2ut/WBC9QAwx4Qo\nAKZea22vJKmqmyS5a7oeqFOq6jdbaydvoe3By63ve6gOGnetAFz3CVFj9PV3jPa7eNdTrj9Su5u9\n+dSR2l116aUjteOatjlgtJFAL33xsSO1u92nnzBSu9u87AsjtYNp1Fo7J8nxVXVykjOTvDPJAZOt\nCoB5Y2IJAGZOa+3bSb6a5Jeq6kaTrgeA+SJEATCrbtYvN020CgDmjhAFwFSqqn2rat0y67fpH7a7\nZ5KTWms/XvvqAJhn7okCYFr9RpJXVtXnknwryXnpZui7V5JbJ/l+kidNrjwA5pUQBcC0+liS26Z7\nJtQdk9wgySXpJpQ4LsnrW2s/mlx5AMwrIQqAqdRaOy3J0yddBwAs5Z4oAACAAYQoAACAAYQoAACA\nAYQoAACAAYQoAACAAczOB8DcOmDvddlw9GGTLgOAGaMnCgAAYAA9UWP0qUNfP1K7ve+300jtjnvy\nXiO1e8sLHzK4zW4nnj7SuTZdeOFI7dbapnsfNLjN7735fSOd6z8vu8VI7fZ92SUjtdvU2kjtAABY\nnp4oAACAAYQoAACAAYQoAACAAYQoAACAAUwsAcDcOu3sC7L+yBMmXcbP2WjadYCppicKAABgACEK\nAABgACEKAABgACEKAABgACEKAABgACEKAABgACEKAABgAM+JGqN7/esfjtTuGw9+00jtHrPr90dr\n97q/Htzmsd++z0jn+sZfHjJSu3PuftVI7Z53738dqd1Dd3nD4DY33GbHkc510F8eMVK7vU4/aaR2\nMKuqao8kD0lyWJLbJ9k7yUQ51j4AAA9DSURBVBVJ/ivJ25K8rbU22g8LALgWhCgAptUjkvx1ku8l\n+WSS7yS5SZKHJnlLkl+vqke01trkSgRgHglRAEyrM5M8KMkJi3ucqur5Sb6Y5GHpAtX7J1MeAPPK\nPVEATKXW2idaa/+ydMhea+37SRbGQd97zQsDYO4JUQDMoiv75U8nWgUAc8lwPgBmSlVtl+R3+i8/\nvBX7b1hh035jKwqAuaInCoBZc3SSA5J8qLX2kUkXA8D80RMFwMyoqmckeU6SM5I8ZmvatNYOXuFY\nG5IcNL7qAJgXeqIAmAlV9fQkr0vy1SSHttZ+NOGSAJhTQhQAU6+qnpXkDUlOSxegRnvaOACMgRAF\nwFSrqj9O8tokp6YLUOdOuCQA5pwQBcDUqqoXpZtIYkOS+7bWfjjhkgDAxBIATKeqemySP0myKcln\nkzyjqpbutrG19vY1Lg2AOSdEATCtbtUvt03yrBX2+XSSt69JNQDQE6LGaP8XfWOkdvte8dSR2h1/\n+DEjtbvtdsNHcb7jlp8Y6Vz5sxHbjeiydsVI7b78k10Ht/njo35/pHPtddxJI7WDedNaOyrJURMu\nAwB+jnuiAAAABhCiAAAABhCiAAAABhCiAAAABhCiAAAABjA7HwBz64C912XD0YdNugwAZoyeKAAA\ngAGEKAAAgAGEKAAAgAGEKAAAgAGEKAAAgAGEKAAAgAFMcQ7A3Drt7Auy/sgT1vy8G02rDjDThKgx\n2nTej0Zqd9tn/ftI7f7oWYeM1O6qux84uM2lN7v+SOdaa7t+65KR2rUv/dfgNjfIF0Y6FwAAs81w\nPgAAgAGEKAAAgAGEKAAAgAGEKAAAgAGEKAAAgAGEKAAAgAGEKACmUlU9vKreUFWfraoLq6pV1bsm\nXRcAeE4UANPqhUnukOTiJGcl2W+y5QBAR08UANPq2Un2TbJbkqdMuBYA+Bk9UQBMpdbaJxf+XFWT\nLAUArkFPFAAAwAB6ogC4TquqDStsco8VACPREwUAADCAnqg5tM3nTh3cZpdVqGM1tEkXAEyd1trB\ny63ve6gOWuNyALgO0BMFAAAwgBAFAAAwgBAFAAAwgBAFAAAwgIklAJhKVXV4ksP7L/fql3epqrf3\nf/5ha+25a14YAHNPiAJgWh2Y5LFL1t26fyXJt5MIUQCsOcP5AJhKrbWjWmu1mdf6SdcIwHwSogAA\nAAYQogAAAAYQogAAAAYQogAAAAYQogAAAAYwxTkAc+uAvddlw9GHTboMAGaMnigAAIABhCgAAIAB\nhCgAAIABhCgAAIABhCgAAIABhCgAAIABTHEOwNw67ewLsv7IE9b0nBtNqQ4w8/REAQAADCBEAQAA\nDCBEAQAADCBEAQAADCBEAQAADCBEAQAADCBEAQAADCBEATC1qurmVXVsVX23qn5SVRur6piquuGk\nawNgfnnYLgBTqapuk+SkJHsm+WCSM5LcOckzkzygqu7WWjtvgiUCMKf0RAEwrf4qXYB6Rmvt8Nba\nka21+yR5bZLbJXnFRKsDYG4JUQBMnb4X6v5JNib5yyWbX5LkkiSPqaqd17g0ABCiAJhKh/bLE1tr\nVy3e0Fq7KMnnk+yU5JC1LgwA3BMFwDS6Xb88c4XtX0/XU7Vvko9v7kBVtWGFTfuNVhoA805PFADT\naF2/vGCF7Qvrb7AGtQDANeiJAuA6rbV28HLr+x6qg9a4HACuA/REATCNFnqa1q2wfWH9+WtQCwBc\ngxAFwDT6Wr/cd4Xt+/TLle6ZAoBVI0QBMI0+2S/vX1XX+F1VVbsmuVuSS5P8+1oXBgBCFABTp7X2\nP0lOTLI+ydOWbH5pkp2THNdau2SNSwMAE0sAMLWemuSkJK+vqvsmOT3Jr6R7htSZSV4wwdoAmGN6\nogCYSn1v1P9J8vZ04ek5SW6T5HVJDmmtnTe56gCYZ3qiAJharbX/TfL4SdcBAIvpiQIAABhAiAIA\nABhAiAIAABhAiAIAABhAiAIAABjA7HwAzK0D9l6XDUcfNukyAJgxeqIAAAAGEKIAAAAGEKIAAAAG\nEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIAAAAGEKIA\nAAAGEKIAAAAGEKIAAAAGEKIAAAAG2G7SBQDAhKw//fTTc/DBB0+6DgAm4PTTT0+S9aO0FaIAmFe7\nXHbZZZtOPvnk/5x0IVNmv355xkSrmD6uy8pcm+W5LsubpuuyPsmFozQUogCYV6clSWtNV9QiVbUh\ncV2Wcl1W5tosz3VZ3nXlurgnCgAAYICRe6I+etV7a5yFAAAAzAI9UQAAAAMIUQAAAAMIUQAAAANU\na23SNQAAAMwMPVEAAAADCFEAAAADCFEAAAADCFEAAAADCFEAAAADCFEAAAADCFEAAAADCFEAXCdU\n1c2r6tiq+m5V/aSqNlbVMVV1w4HH2b1vt7E/znf74958tWpfbdf22lTVzlX16Kr6u6o6o6ouqaqL\nqurLVfWcqtp+td/DahjX98ySY96zqjZVVauql4+z3rUyzutSVQf13zdn9cc6p6o+XVW/sxq1r6Yx\n/oy5e1V9sG9/eVV9p6o+VFUPWK3aV0tVPbyq3lBVn62qC/vv+3eNeKyx/3tcTR62C8DMq6rbJDkp\nyZ5JPpjkjCR3TnJokq8luVtr7bytOM4e/XH2TfKJJF9Ksl+SByc5N8ldWmvfXI33sFrGcW36D3f/\nluRHST6Z5BtJbpjkQUn26o9/39ba5av0NsZuXN8zS465a5KvJLlRkl2SvKK19sJx1r3axnldqurp\nSV6X5MdJTkhydpLdkxyQ5KzW2iPH/gZWyRh/xjwlyV8luSTJ8UnOSnLzJA9NslOSF7bWXrEa72E1\nVNWpSe6Q5OJ072W/JO9urR0x8Dhj//e46lprXl5eXl5eM/1K8pEkLckfLFn/F/36N23lcf6m3//P\nl6x/Rr/+w5N+r5O4NkkOTPLoJNsvWb9rkg39cZ4z6fc6ie+ZJW2PTRc0n98f4+WTfp+Tui5J7p/k\nqv54uy6z/XqTfq9rfV2SXC/J+UkuS3K7Jdv2T3J5kkuT7DDp9zvguhyaZJ8kleTe/bV416S+79by\npScKgJnW/w/mN5JsTHKb1tpVi7btmuR76X7B79lau2Qzx9klXW/TVUlu2lq7aNG2bZJ8M8kt+3PM\nRG/UuK7NFs7xqCTvTvKvrbUHXuui18BqXJeqenCSDyR5TJLtkrwtM9YTNc7rUlX/meS2SW7Rpq0H\nYaAx/oy5SZLvJ/lKa+0Oy2z/SpLbJ7nRLF6zqrp3up7qQT1Ra/FzajW4JwqAWXdovzxx8S/fJOmD\n0OfTDZM5ZAvHOSTJjkk+vzhA9cdZ+B/1xeebBeO6NptzZb/86bU4xlob63Wpqj2T/G2SD7TWRrof\nZEqM5bpU1QFJfjnJiUl+VFWHVtVz+/vn7tv/p8QsGdf3y7lJfpBk36raZ/GGqto3XY/OqbMYoK6l\ntfg5NXaz9k0MAEvdrl+eucL2r/fLfdfoONNkLd7TE/rlh6/FMdbauK/L36b7TPXka1PUFBjXdblT\nvzw3yafS3V/46iSvSfKxJKdW1W1HL3PNjeW6tG7419PSfa9sqKp3VNUrq+qd6YbF/neSR4yh3lkz\nkz97t5t0AQBwLa3rlxessH1h/Q3W6DjTZFXfUz9xwAOSnJrufqBZMbbrUlVPSDfBxm+31s4ZQ22T\nNK7rsme/fGK6ySQOS/K5JDdJ8uIkRyQ5oapu31q7YvRy18zYvl9aa++tqu8m+fski2coPCfdENCZ\nGCo8ZjP5s1dPFAAwWFU9NMkx6e7xeFhr7cotNLnOqar16a7Be1tr/zjZaqbKwufLbZM8srX2odba\nha21r6cLDl9O16vwsEkVOClVdUS63rjPpptMYqd++fEkb0zyD5OrjiGEKABm3cL/Uq5bYfvC+vPX\n6DjTZFXeU1Udnu7D3rlJ7j0rE20sMq7rcmy6mdaeOo6ipsC4rsvC9u+31r6weEM/pO2D/Zd3Hlzh\nZIzluvT3PR2bbtjeY1prZ7TWLmutnZFuQpINSR7RT9AwT2byZ68QBcCs+1q/XGm8/MIN3CuNtx/3\ncabJ2N9TVT0iyXvTDT+6V2vta1toMo3GdV0OSjd07Qf9Q0ZbVbV0w7KS5AX9ug9cu3LXzLj/La30\noffH/XLHraxr0sZ1Xe6fbprzTy8zgcJVST7Tf3nwKEXOsJn82eueKABm3Sf75f2raptlpse9W7pn\nr/z7Fo7z7+l6Fe5WVbsuM8X5/ZecbxaM69ostHl0kneku8/l0BnsgVowruvyznTDsZbaJ8k9090r\ntiHJKde64rUxzn9LlyRZX1U7LzMt9QH98ltjqHktjOu67NAvb7zC9oX1s3Cf2DiN9efUWtETBcBM\na639T7qplNenm/lqsZcm2TnJcYs/yFXVflW135LjXJzkuH7/o5Yc5+n98T8yS8FhXNemX//YdKHh\nO0nuOUvXYakxfs88o7X2u0tfubon6oR+3V+u2psZozFel0uTvDXJ9ZO8vKpq0f63T/K4dFPiv2/8\n72L8xvjv6LP98uFV9cuLN1TVgUkenu7Bsp8YX/XTo6qu11+X2yxeP8r1nQYetgvAzOt/KZ+UbmjV\nB5OcnuRX0j1/5Mwkd1387JV+yFVaa7XkOHv0x9k33QeZL6a76fvB6e7/uWv/C39mjOPaVNWh6W6G\n3ybdPR3/u8ypzm+tHbNKb2PsxvU9s8KxH5cZfNhuMtZ/S7sl+XSSA5P8R7pn/dwkyUPTDeN7Vmvt\ndav9fsZljNfl2CSPT9fbdHySb6cLD4cn2T7JMa21Z6/y2xmb/v7Iw/sv90rya+lmGFwIjD9srT23\n33d9ut7Hb7fW1i85zqDrOw2EKACuE6rqF5L8Sbopt/dI95T745O8tLX24yX7rviBuKp2T/KSdB8M\nbprkvCT/luTFrbWzVvM9rJZre20WhYLN+bkPRtNuXN8zyxz3cZnREJWM9d/SLkmel+7ZR7dMN1z2\ni0le01o7cTXfw2oYx3Xpe+Uem6437g5Jdk1yYbohn3/bWpup2fmq6qh0Py9X8rOfC5sLUf32rb6+\n00CIAgAAGMA9UQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMI\nUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAMIUQAAAAP8f4F4\n0joG/xM1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 424,
              "height": 235
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUBGm2NSQniq"
      },
      "source": [
        "The operations are availble by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you'd use model[0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QyFsLFJQpyn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ae382f4d-7b94-452d-bdc0-694bed7c1c3d"
      },
      "source": [
        "print(model[0])\n",
        "model[0].weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=128, bias=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0152, -0.0294,  0.0026,  ...,  0.0148,  0.0261, -0.0223],\n",
              "        [ 0.0032,  0.0163,  0.0208,  ..., -0.0124, -0.0124,  0.0186],\n",
              "        [ 0.0315, -0.0142, -0.0310,  ..., -0.0192, -0.0179,  0.0007],\n",
              "        ...,\n",
              "        [-0.0064, -0.0138, -0.0133,  ..., -0.0205, -0.0312,  0.0161],\n",
              "        [ 0.0131,  0.0127, -0.0273,  ...,  0.0140, -0.0335, -0.0295],\n",
              "        [ 0.0213, -0.0146, -0.0260,  ..., -0.0235,  0.0335, -0.0196]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Sequential with OrderedDict"
      ],
      "metadata": {
        "id": "5cl3XcYWSm-H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLR8vOZQwMR"
      },
      "source": [
        "You can also pass in an OrderedDict to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so each operation must have a different name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-BxTVvRQzau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "59647cf4-6fd2-491e-a4d0-0530d27b474e"
      },
      "source": [
        "from collections import OrderedDict\n",
        "model = nn.Sequential(OrderedDict([\n",
        "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
        "                      ('relu1', nn.ReLU()),\n",
        "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
        "                      ('relu2', nn.ReLU()),\n",
        "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
        "                      ('softmax', nn.Softmax(dim=1))]))\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT967o7DQ10C"
      },
      "source": [
        "Now you can access layers either by integer or the name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OuEUDnXQ3Lz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33d2555a-a738-4362-af81-03d87b685f19"
      },
      "source": [
        "print(model[0])\n",
        "print(model.fc1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=128, bias=True)\n",
            "Linear(in_features=784, out_features=128, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI1N25R3Q9GD"
      },
      "source": [
        "## Training Neural Networks via nn.NLLLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BVKHqbfRlzV"
      },
      "source": [
        "Training the network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUCzeZXyT5So"
      },
      "source": [
        "it's more convenient to build the model with a log-softmax output using nn.LogSoftmax or F.log_softmax (documentation). Then you can get the actual probabilites by taking the exponential torch.exp(output). With a log-softmax output, you want to use the negative log likelihood loss, nn.NLLLoss (documentation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3B9mMGJRnli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "551d5468-aadb-41f9-9116-2769fe05ec8d"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 1.91916799659668\n",
            "Training loss: 0.8666147714230552\n",
            "Training loss: 0.5323591251363123\n",
            "Training loss: 0.4345304797421386\n",
            "Training loss: 0.3882274371283903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNJPPjg1S3yB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "309a62c0-ed29-40f2-cdde-4202df712282"
      },
      "source": [
        "%matplotlib inline\n",
        "import helper\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAHXCAYAAABd89BGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAGEKAAAgAGEKAAAgAH+P1CPwjcQ7iL1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 424,
              "height": 235
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST with L1 norm"
      ],
      "metadata": {
        "id": "-pF3ytvoTv5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "G1zHndErT3Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Prepare CIFAR-10 dataset\n",
        "dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl7XvmEzTyAv",
        "outputId": "2abcc9e4-6210-4a74-e9e4-0ab4cdb11bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9912422/9912422 [00:00<00:00, 158307674.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 28881/28881 [00:00<00:00, 41399758.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1648877/1648877 [00:00<00:00, 58524438.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4542/4542 [00:00<00:00, 3374761.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l1_loss(self, w):\n",
        "      return torch.abs(w).sum()"
      ],
      "metadata": {
        "id": "K0wx7MwOUEEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "#loss_function = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3) #lr=1e-4\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "loss_function = nn.NLLLoss() #nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=0.003)"
      ],
      "metadata": {
        "id": "u9rRBAjjUH-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # Get inputs\n",
        "    inputs, targets = data\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "\n",
        "    # Compute L1 loss component\n",
        "    l1_weight = 0.1 #1.0\n",
        "    l1_parameters = []\n",
        "    for parameter in mlp.parameters():\n",
        "        l1_parameters.append(parameter.view(-1))\n",
        "    l1 = l1_weight * mlp.compute_l1_loss(torch.cat(l1_parameters))\n",
        "\n",
        "    # Add L1 loss component\n",
        "    loss += l1\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    minibatch_loss = loss.item()\n",
        "    if i % 500 == 499:\n",
        "        print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss)' %\n",
        "              (i + 1, minibatch_loss, l1))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8wpMNQUUQJE",
        "outputId": "1a61d24b-4948-4674-c8db-d4b1fe899402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 0.89534 (of which 0.91890 L1 loss)\n",
            "Loss after mini-batch  1000: 0.79132 (of which 0.85138 L1 loss)\n",
            "Loss after mini-batch  1500: 0.78278 (of which 0.85333 L1 loss)\n",
            "Loss after mini-batch  2000: 0.76488 (of which 0.85443 L1 loss)\n",
            "Loss after mini-batch  2500: 0.76180 (of which 0.85543 L1 loss)\n",
            "Loss after mini-batch  3000: 0.78230 (of which 0.85660 L1 loss)\n",
            "Loss after mini-batch  3500: 0.77853 (of which 0.85785 L1 loss)\n",
            "Loss after mini-batch  4000: 0.75869 (of which 0.85816 L1 loss)\n",
            "Loss after mini-batch  4500: 0.82480 (of which 0.85936 L1 loss)\n",
            "Loss after mini-batch  5000: 0.80073 (of which 0.86069 L1 loss)\n",
            "Loss after mini-batch  5500: 0.77297 (of which 0.86392 L1 loss)\n",
            "Loss after mini-batch  6000: 0.76496 (of which 0.86530 L1 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 0.80290 (of which 0.86718 L1 loss)\n",
            "Loss after mini-batch  1000: 0.75021 (of which 0.86947 L1 loss)\n",
            "Loss after mini-batch  1500: 0.73842 (of which 0.87238 L1 loss)\n",
            "Loss after mini-batch  2000: 0.77199 (of which 0.87497 L1 loss)\n",
            "Loss after mini-batch  2500: 0.78227 (of which 0.87757 L1 loss)\n",
            "Loss after mini-batch  3000: 0.78909 (of which 0.87957 L1 loss)\n",
            "Loss after mini-batch  3500: 0.77931 (of which 0.88269 L1 loss)\n",
            "Loss after mini-batch  4000: 0.70826 (of which 0.88444 L1 loss)\n",
            "Loss after mini-batch  4500: 0.75349 (of which 0.88590 L1 loss)\n",
            "Loss after mini-batch  5000: 0.83172 (of which 0.88822 L1 loss)\n",
            "Loss after mini-batch  5500: 0.81958 (of which 0.89125 L1 loss)\n",
            "Loss after mini-batch  6000: 0.72161 (of which 0.89445 L1 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 0.82348 (of which 0.89693 L1 loss)\n",
            "Loss after mini-batch  1000: 0.72175 (of which 0.89934 L1 loss)\n",
            "Loss after mini-batch  1500: 0.75920 (of which 0.90228 L1 loss)\n",
            "Loss after mini-batch  2000: 0.85397 (of which 0.90392 L1 loss)\n",
            "Loss after mini-batch  2500: 0.84270 (of which 0.90532 L1 loss)\n",
            "Loss after mini-batch  3000: 0.76924 (of which 0.90718 L1 loss)\n",
            "Loss after mini-batch  3500: 0.81098 (of which 0.91035 L1 loss)\n",
            "Loss after mini-batch  4000: 0.80262 (of which 0.91274 L1 loss)\n",
            "Loss after mini-batch  4500: 0.81656 (of which 0.91556 L1 loss)\n",
            "Loss after mini-batch  5000: 0.73589 (of which 0.91964 L1 loss)\n",
            "Loss after mini-batch  5500: 0.89981 (of which 0.92223 L1 loss)\n",
            "Loss after mini-batch  6000: 0.87121 (of which 0.92500 L1 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 0.71782 (of which 0.92703 L1 loss)\n",
            "Loss after mini-batch  1000: 0.80789 (of which 0.92926 L1 loss)\n",
            "Loss after mini-batch  1500: 0.77687 (of which 0.93287 L1 loss)\n",
            "Loss after mini-batch  2000: 0.81731 (of which 0.93587 L1 loss)\n",
            "Loss after mini-batch  2500: 0.85529 (of which 0.93876 L1 loss)\n",
            "Loss after mini-batch  3000: 0.69475 (of which 0.94194 L1 loss)\n",
            "Loss after mini-batch  3500: 0.72821 (of which 0.94440 L1 loss)\n",
            "Loss after mini-batch  4000: 0.90623 (of which 0.94604 L1 loss)\n",
            "Loss after mini-batch  4500: 0.74034 (of which 0.94743 L1 loss)\n",
            "Loss after mini-batch  5000: 0.79906 (of which 0.95068 L1 loss)\n",
            "Loss after mini-batch  5500: 0.51570 (of which 0.95435 L1 loss)\n",
            "Loss after mini-batch  6000: 0.90734 (of which 0.95771 L1 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 0.84283 (of which 0.96185 L1 loss)\n",
            "Loss after mini-batch  1000: 0.86331 (of which 0.96461 L1 loss)\n",
            "Loss after mini-batch  1500: 0.78513 (of which 0.96654 L1 loss)\n",
            "Loss after mini-batch  2000: 0.89947 (of which 0.96923 L1 loss)\n",
            "Loss after mini-batch  2500: 0.70580 (of which 0.97115 L1 loss)\n",
            "Loss after mini-batch  3000: 0.91668 (of which 0.97362 L1 loss)\n",
            "Loss after mini-batch  3500: 0.82416 (of which 0.97772 L1 loss)\n",
            "Loss after mini-batch  4000: 0.69839 (of which 0.98055 L1 loss)\n",
            "Loss after mini-batch  4500: 0.78368 (of which 0.98260 L1 loss)\n",
            "Loss after mini-batch  5000: 0.63840 (of which 0.98470 L1 loss)\n",
            "Loss after mini-batch  5500: 0.75028 (of which 0.98733 L1 loss)\n",
            "Loss after mini-batch  6000: 0.95719 (of which 0.98985 L1 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST with L2 norm"
      ],
      "metadata": {
        "id": "1fnh56jhcILH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l2_loss(self, w):\n",
        "      return torch.square(w).sum()"
      ],
      "metadata": {
        "id": "pnq9nGZxcKNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp2 = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "mmErnavkcRvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # Get inputs\n",
        "    inputs, targets = data\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp2(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "\n",
        "    # Compute l2 loss component\n",
        "    l2_weight = 1.0\n",
        "    l2_parameters = []\n",
        "    for parameter in mlp.parameters():\n",
        "        l2_parameters.append(parameter.view(-1))\n",
        "    l2 = l2_weight * mlp2.compute_l2_loss(torch.cat(l2_parameters))\n",
        "\n",
        "    # Add l2 loss component\n",
        "    loss += l2\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    minibatch_loss = loss.item()\n",
        "    if i % 500 == 499:\n",
        "        print('Loss after mini-batch %5d: %.5f (of which %.5f l2 loss)' %\n",
        "              (i + 1, minibatch_loss, l2))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8PZGdx6cX9M",
        "outputId": "9616a4f8-f61c-4666-8775-094867cccf3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 2.28985 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1000: 2.33224 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1500: 2.29291 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2000: 2.35104 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2500: 2.40571 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3000: 2.32717 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3500: 2.34617 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4000: 2.35007 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4500: 2.26785 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5000: 2.27904 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5500: 2.28733 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  6000: 2.34296 (of which 0.00000 l2 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.32021 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1000: 2.25215 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1500: 2.26117 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2000: 2.33205 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30601 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3000: 2.28638 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3500: 2.28610 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4000: 2.27601 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4500: 2.34327 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5000: 2.31752 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30984 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  6000: 2.39672 (of which 0.00000 l2 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.31689 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30470 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30414 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2000: 2.31271 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2500: 2.27290 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3000: 2.28910 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3500: 2.25034 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4000: 2.25874 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4500: 2.36190 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5000: 2.27021 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5500: 2.29700 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  6000: 2.39115 (of which 0.00000 l2 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.34794 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1000: 2.31431 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1500: 2.28480 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2000: 2.26846 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2500: 2.33563 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3000: 2.31080 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3500: 2.28333 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4000: 2.29260 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4500: 2.32124 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5000: 2.31609 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5500: 2.34066 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  6000: 2.26887 (of which 0.00000 l2 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.29404 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1000: 2.31633 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  1500: 2.34334 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2000: 2.32537 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  2500: 2.36943 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3000: 2.34235 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  3500: 2.36184 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4000: 2.31933 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  4500: 2.34067 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5000: 2.32128 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  5500: 2.31978 (of which 0.00000 l2 loss)\n",
            "Loss after mini-batch  6000: 2.37059 (of which 0.00000 l2 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elastic Net (L1+L2) Regularization with PyTorch"
      ],
      "metadata": {
        "id": "7KdA5u6GdZcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "  def compute_l1_loss(self, w):\n",
        "      return torch.abs(w).sum()\n",
        "\n",
        "  def compute_l2_loss(self, w):\n",
        "      return torch.square(w).sum()"
      ],
      "metadata": {
        "id": "B4fcXOqPdasa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "2zVpBywcdmk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # Get inputs\n",
        "    inputs, targets = data\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "\n",
        "    # Specify L1 and L2 weights\n",
        "    l1_weight = 0.3\n",
        "    l2_weight = 0.7\n",
        "\n",
        "    # Compute L1 and L2 loss component\n",
        "    parameters = []\n",
        "    for parameter in mlp.parameters():\n",
        "        parameters.append(parameter.view(-1))\n",
        "    l1 = l1_weight * mlp.compute_l1_loss(torch.cat(parameters))\n",
        "    l2 = l2_weight * mlp.compute_l2_loss(torch.cat(parameters))\n",
        "\n",
        "    # Add L1 and L2 loss components\n",
        "    loss += l1\n",
        "    loss += l2\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    minibatch_loss = loss.item()\n",
        "    if i % 500 == 499:\n",
        "        print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss; %0.5f L2 loss)' %\n",
        "              (i + 1, minibatch_loss, l1, l2))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs3tRjG8dp-2",
        "outputId": "bc2187d5-e104-40ed-e2f7-63e981f60132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 24.79208 (of which 19.76384 L1 loss; 2.71377 L2 loss)\n",
            "Loss after mini-batch  1000: 7.45958 (of which 4.76212 L1 loss; 0.38665 L2 loss)\n",
            "Loss after mini-batch  1500: 2.98764 (of which 0.65812 L1 loss; 0.02930 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48230 (of which 0.17971 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48037 (of which 0.17778 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.47990 (of which 0.17730 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.47955 (of which 0.17696 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.47973 (of which 0.17713 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.47883 (of which 0.17624 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.47921 (of which 0.17663 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48054 (of which 0.17796 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48066 (of which 0.17808 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.48047 (of which 0.17788 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48027 (of which 0.17768 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48057 (of which 0.17797 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48184 (of which 0.17925 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48275 (of which 0.18016 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48287 (of which 0.18029 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48428 (of which 0.18169 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48569 (of which 0.18310 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48625 (of which 0.18365 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48743 (of which 0.18484 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48711 (of which 0.18451 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48683 (of which 0.18424 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.48787 (of which 0.18528 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48779 (of which 0.18520 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48710 (of which 0.18449 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48744 (of which 0.18485 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48799 (of which 0.18541 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48777 (of which 0.18517 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48768 (of which 0.18509 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48720 (of which 0.18461 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48697 (of which 0.18438 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48784 (of which 0.18524 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48795 (of which 0.18535 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48758 (of which 0.18499 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.48798 (of which 0.18538 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48827 (of which 0.18566 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48763 (of which 0.18503 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48738 (of which 0.18479 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48710 (of which 0.18451 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48675 (of which 0.18415 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48818 (of which 0.18559 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48841 (of which 0.18582 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48750 (of which 0.18492 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48786 (of which 0.18527 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48803 (of which 0.18543 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48742 (of which 0.18482 L1 loss; 0.00001 L2 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.48741 (of which 0.18481 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1000: 2.48731 (of which 0.18471 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  1500: 2.48698 (of which 0.18439 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2000: 2.48835 (of which 0.18576 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  2500: 2.48813 (of which 0.18553 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3000: 2.48722 (of which 0.18463 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  3500: 2.48784 (of which 0.18525 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4000: 2.48790 (of which 0.18531 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  4500: 2.48763 (of which 0.18504 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5000: 2.48771 (of which 0.18512 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  5500: 2.48731 (of which 0.18472 L1 loss; 0.00001 L2 loss)\n",
            "Loss after mini-batch  6000: 2.48710 (of which 0.18450 L1 loss; 0.00001 L2 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST with NLLLoss and weight_decay"
      ],
      "metadata": {
        "id": "PeVXSjRwdJv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 10),\n",
        "      nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "O64ZNo2IfCxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss() #nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=0.003, weight_decay=0.1)\n",
        "#optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1.0)\n"
      ],
      "metadata": {
        "id": "WTlXstdgdLZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  running_loss = 0\n",
        "\n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # Get inputs\n",
        "    inputs, targets = data\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    minibatch_loss = loss.item()\n",
        "\n",
        "    running_loss += minibatch_loss\n",
        "    if i % 500 == 499:\n",
        "        print('Loss after mini-batch %5d: %.5f' %\n",
        "              (i + 1, minibatch_loss))\n",
        "        current_loss = 0.0\n",
        "\n",
        "  print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbN7ffzcfdh_",
        "outputId": "8d9e9205-4033-4826-901e-a4fd8c39a9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 2.29625\n",
            "Loss after mini-batch  1000: 2.26610\n",
            "Loss after mini-batch  1500: 2.25018\n",
            "Loss after mini-batch  2000: 2.21213\n",
            "Loss after mini-batch  2500: 2.27765\n",
            "Loss after mini-batch  3000: 2.22091\n",
            "Loss after mini-batch  3500: 2.10362\n",
            "Loss after mini-batch  4000: 2.01979\n",
            "Loss after mini-batch  4500: 1.97962\n",
            "Loss after mini-batch  5000: 1.94818\n",
            "Loss after mini-batch  5500: 1.72742\n",
            "Loss after mini-batch  6000: 1.62884\n",
            "Training loss: 2.1073102538784343\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 1.58494\n",
            "Loss after mini-batch  1000: 1.54844\n",
            "Loss after mini-batch  1500: 1.47698\n",
            "Loss after mini-batch  2000: 1.68031\n",
            "Loss after mini-batch  2500: 1.26278\n",
            "Loss after mini-batch  3000: 1.69299\n",
            "Loss after mini-batch  3500: 1.57599\n",
            "Loss after mini-batch  4000: 1.33420\n",
            "Loss after mini-batch  4500: 1.34147\n",
            "Loss after mini-batch  5000: 0.88728\n",
            "Loss after mini-batch  5500: 1.09051\n",
            "Loss after mini-batch  6000: 1.24930\n",
            "Training loss: 1.3009029420117537\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 0.92717\n",
            "Loss after mini-batch  1000: 1.02074\n",
            "Loss after mini-batch  1500: 0.82795\n",
            "Loss after mini-batch  2000: 0.91176\n",
            "Loss after mini-batch  2500: 1.28605\n",
            "Loss after mini-batch  3000: 1.18358\n",
            "Loss after mini-batch  3500: 1.29021\n",
            "Loss after mini-batch  4000: 1.15309\n",
            "Loss after mini-batch  4500: 0.87906\n",
            "Loss after mini-batch  5000: 0.82815\n",
            "Loss after mini-batch  5500: 1.01584\n",
            "Loss after mini-batch  6000: 1.31254\n",
            "Training loss: 1.0302667961021263\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 0.69871\n",
            "Loss after mini-batch  1000: 1.27257\n",
            "Loss after mini-batch  1500: 1.02664\n",
            "Loss after mini-batch  2000: 0.77824\n",
            "Loss after mini-batch  2500: 1.07892\n",
            "Loss after mini-batch  3000: 1.09908\n",
            "Loss after mini-batch  3500: 0.96080\n",
            "Loss after mini-batch  4000: 1.12298\n",
            "Loss after mini-batch  4500: 1.45345\n",
            "Loss after mini-batch  5000: 1.14305\n",
            "Loss after mini-batch  5500: 0.90617\n",
            "Loss after mini-batch  6000: 1.19905\n",
            "Training loss: 1.0061287305603426\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 0.76721\n",
            "Loss after mini-batch  1000: 0.67737\n",
            "Loss after mini-batch  1500: 0.90001\n",
            "Loss after mini-batch  2000: 1.14996\n",
            "Loss after mini-batch  2500: 1.07475\n",
            "Loss after mini-batch  3000: 1.00569\n",
            "Loss after mini-batch  3500: 0.74219\n",
            "Loss after mini-batch  4000: 1.36583\n",
            "Loss after mini-batch  4500: 1.22845\n",
            "Loss after mini-batch  5000: 1.01766\n",
            "Loss after mini-batch  5500: 0.71392\n",
            "Loss after mini-batch  6000: 0.75855\n",
            "Training loss: 1.000247830380996\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "img = images[4].view(1, 784)\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPdvq10xbNR6",
        "outputId": "3fdef934-2712-40dd-8c2e-461135674f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn off gradients to speed up this part\n",
        "with torch.inference_mode():#torch.no_grad():\n",
        "    logps = mlp(img)"
      ],
      "metadata": {
        "id": "Mr7Z1HGkbP2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logps.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99U6wtODbUCs",
        "outputId": "2fe4165c-72d2-4c7a-8c16-ed9a3a20b8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHX8CyPAblCm",
        "outputId": "4b9194aa-91ce-418e-9247-19ce677eb98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-8.1563, -0.2899, -2.4180, -3.3348, -5.1323, -4.9474, -4.8144, -5.0262,\n",
              "         -2.4226, -4.5796]])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)"
      ],
      "metadata": {
        "id": "YsX8tt9kjsLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W6uxwkvjtwJ",
        "outputId": "dbc01963-0f39-4c08-d0b0-0cbde02ec202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.8694e-04, 7.4836e-01, 8.9103e-02, 3.5621e-02, 5.9032e-03, 7.1020e-03,\n",
              "         8.1121e-03, 6.5634e-03, 8.8692e-02, 1.0259e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "EEzdZAUWb2N9",
        "outputId": "e5c82c7a-36e8-47f8-f168-9a5eea8727e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x900 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFICAYAAABN38p2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoe0lEQVR4nO3daXQUZaLG8SckpBNCEjBsiTRb2FcVBmSRxUEyiMh4DquMBBxEJQwiykjG0YAIQUTEixiVywAjQVRGYGZEIjAsBwHZFWSTPYiAciEJWwPJez946LFNQopAupru/++c+tDVb1U/1ZHk8a3q6iBjjBEAAACuq5TdAQAAAG4HlCYAAAALKE0AAAAWUJoAAAAsoDQBAABYQGkCAACwgNIEAABgAaUJAADAAkoTAACABZQmAICHGjVqaODAgXbHsE1QUJCGDRt2y/Y3e/ZsBQUFafPmzUWO7dixozp27Oh+fPjwYQUFBWn27NnudWPGjFFQUNAtywfrKE0AECAOHDigJ598UrVq1VJYWJiioqLUtm1bvfXWW7p48aLd8a7rWvG4toSFhalu3boaNmyYTp48aXc8202YMEGLFi2yO4bfC7E7AACg5H322Wfq1auXHA6HBgwYoMaNG+vy5ctau3atRo0apW+//Vbvv/++3TGL9Morr6hmzZq6dOmS1q5dq7S0NC1ZskQ7d+5UmTJl7I5307744osix/z1r3/V6NGjPdZNmDBBPXv21O9///sSSgaJ0gQAfu/QoUPq27evqlevrv/85z+KjY11P5eUlKT9+/frs88+szGhdV27dlWLFi0kSYMHD1ZMTIymTJmixYsXq1+/fgVuc/78eUVERHgzZrGFhoYWOSYkJEQhIfz5tgOn5wDAz02aNEnnzp3TzJkzPQrTNbVr19YzzzxT6Pb/93//p+eff15NmjRR2bJlFRUVpa5du+rrr7/ON3batGlq1KiRypQpo/Lly6tFixaaN2+e+/mcnByNGDFCNWrUkMPhUKVKlfTAAw9o69atxTq2+++/X9LPxVCSBg4cqLJly+rAgQN68MEHFRkZqf79+0v6uTw999xzcjqdcjgcqlevniZPnixjTIH7Tk9PV7169RQWFqbmzZtrzZo1Hs8fOXJEQ4cOVb169RQeHq6YmBj16tVLhw8fLnB/Fy5c0JNPPqmYmBhFRUVpwIABOnPmjMeYX1/TVJBfX9MUFBSk8+fPa86cOe7TlwMHDtTKlSsVFBSkhQsX5tvHvHnzFBQUpPXr11/3teCJqgoAfu5f//qXatWqpTZt2hRr+4MHD2rRokXq1auXatasqZMnT+q9995Thw4dtGvXLsXFxUmSZsyYoeHDh6tnz5565plndOnSJX3zzTf66quv9Oijj0qSnnrqKS1YsEDDhg1Tw4YNdfr0aa1du1a7d+/WPffcc8PZDhw4IEmKiYlxr7t69aoSEhLUrl07TZ48WWXKlJExRg8//LBWrlypP/7xj7rrrruUkZGhUaNG6fvvv9ebb77psd/Vq1fro48+0vDhw+VwOPTOO+/od7/7nTZu3KjGjRtLkjZt2qR169apb9++qlq1qg4fPqy0tDR17NhRu3btyne6cNiwYSpXrpzGjBmjvXv3Ki0tTUeOHNGqVatu6sLuDz74QIMHD1bLli01ZMgQSVJ8fLzuvfdeOZ1Opaen65FHHvHYJj09XfHx8WrdunWxXzcgGQCA38rKyjKSTI8ePSxvU716dZOYmOh+fOnSJZObm+sx5tChQ8bhcJhXXnnFva5Hjx6mUaNG1913dHS0SUpKspzlmlmzZhlJZvny5ebHH380mZmZZv78+SYmJsaEh4ebY8eOGWOMSUxMNJLM6NGjPbZftGiRkWReffVVj/U9e/Y0QUFBZv/+/e51kowks3nzZve6I0eOmLCwMPPII4+41124cCFfzvXr1xtJ5u9//3u+7M2bNzeXL192r580aZKRZBYvXuxe16FDB9OhQwf340OHDhlJZtasWe51KSkp5td/viMiIjx+ZtckJycbh8Nhzp4961536tQpExISYlJSUvKNx/Vxeg4A/Fh2drYkKTIystj7cDgcKlXq5z8Xubm5On36tMqWLat69ep5nFYrV66cjh07pk2bNhW6r3Llyumrr77S8ePHi5Wlc+fOqlixopxOp/r27auyZctq4cKFuvPOOz3GPf300x6PlyxZouDgYA0fPtxj/XPPPSdjjD7//HOP9a1bt1bz5s3dj6tVq6YePXooIyNDubm5kqTw8HD381euXNHp06dVu3ZtlStXrsDTjUOGDFHp0qU9MoaEhGjJkiU3+C5YN2DAALlcLi1YsMC97qOPPtLVq1f1hz/8ocRe119RmgDAj0VFRUn6+Vqi4srLy9Obb76pOnXqyOFwqEKFCqpYsaK++eYbZWVluce98MILKlu2rFq2bKk6deooKSlJX375pce+Jk2apJ07d8rpdKply5YaM2aMDh48aDnL9OnTtWzZMq1cuVK7du3SwYMHlZCQ4DEmJCREVatW9Vh35MgRxcXF5SuPDRo0cD//S3Xq1Mn32nXr1tWFCxf0448/SpIuXryol19+2X2N1LX35ezZsx7vS2H7LFu2rGJjYwu9BupWqF+/vn7zm98oPT3dvS49PV333nuvateuXWKv668oTQDgx6KiohQXF6edO3cWex8TJkzQyJEj1b59e82dO1cZGRlatmyZGjVqpLy8PPe4Bg0aaO/evZo/f77atWunf/zjH2rXrp1SUlLcY3r37q2DBw9q2rRpiouL0+uvv65GjRrlm+kpTMuWLdW5c2d17NhRDRo0cM+A/dIvZ8ZK0p/+9CeNHz9evXv31scff6wvvvhCy5YtU0xMjMf7YrcBAwZo9erVOnbsmA4cOKANGzYwy1RMlCYA8HMPPfSQDhw4UOxPSi1YsECdOnXSzJkz1bdvX3Xp0kWdO3fW2bNn842NiIhQnz59NGvWLB09elTdunXT+PHjdenSJfeY2NhYDR06VIsWLdKhQ4cUExOj8ePHF/fwLKlevbqOHz+eb8Ztz5497ud/6bvvvsu3j3379qlMmTKqWLGipJ/fl8TERL3xxhvq2bOnHnjgAbVr167A96WgfZ47d04//PCDatSoUcyj+q/rXUjet29fBQcH68MPP1R6erpKly6tPn363PRrBiJKEwD4uT//+c+KiIjQ4MGDC7x79oEDB/TWW28Vun1wcHC+j+V/8skn+v777z3WnT592uNxaGioGjZsKGOMrly5otzc3HynrSpVqqS4uDi5XK4bPawb8uCDDyo3N1dvv/22x/o333xTQUFB6tq1q8f69evXe1yXlJmZqcWLF6tLly4KDg6WVPD7Mm3aNPc1T7/2/vvv68qVK+7HaWlpunr1ar7XLo6IiIhCy1qFChXUtWtXzZ07V+np6frd736nChUq3PRrBiJuOQAAfi4+Pl7z5s1Tnz591KBBA487gq9bt06ffPLJdb9r7qGHHtIrr7yiQYMGqU2bNtqxY4fS09NVq1Ytj3FdunRRlSpV1LZtW1WuXFm7d+/W22+/rW7duikyMlJnz55V1apV1bNnTzVr1kxly5bV8uXLtWnTJr3xxhsl+h50795dnTp10osvvqjDhw+rWbNm+uKLL7R48WKNGDFC8fHxHuMbN26shIQEj1sOSNLYsWM93pcPPvhA0dHRatiwodavX6/ly5d73P7gly5fvqzf/va36t27t/bu3at33nlH7dq108MPP3zTx9e8eXMtX75cU6ZMUVxcnGrWrKlWrVq5nx8wYIB69uwpSRo3btxNv17AsvfDewAAb9m3b5954oknTI0aNUxoaKiJjIw0bdu2NdOmTTOXLl1yjyvolgPPPfeciY2NNeHh4aZt27Zm/fr1+T4e/95775n27dubmJgY43A4THx8vBk1apTJysoyxhjjcrnMqFGjTLNmzUxkZKSJiIgwzZo1M++8806R2a99bH/Tpk3XHZeYmGgiIiIKfC4nJ8c8++yzJi4uzpQuXdrUqVPHvP766yYvL89jnCSTlJRk5s6da+rUqWMcDoe5++67zcqVKz3GnTlzxgwaNMhUqFDBlC1b1iQkJJg9e/bke/+uZV+9erUZMmSIKV++vClbtqzp37+/OX36tMc+i3vLgT179pj27dub8PBwIynf7QdcLpcpX768iY6ONhcvXrzue4jCBRlTyK1QAQCAX7h69ari4uLUvXt3zZw50+44ty2uaQIAwM8tWrRIP/74owYMGGB3lNsaM00AAPipr776St98843GjRunChUqFPs7/vAzZpoAAPBTaWlpevrpp1WpUiX9/e9/tzvObY+ZJgAAAAss33LggVK9SjIHAD+zLO8TuyMAwC3FfZoA+KW8vDwdP35ckZGR171bMgAYY5STk6O4uLjrfgUPpQmAXzp+/LicTqfdMQDcRjIzM/N92fMvUZoA+KVr32afmZmpqKgom9MA8GXZ2dlyOp3u3xuFoTQB8EvXTslFRUVRmgBYUtSpfG45AAAAYAGlCQAAwAJKEwAAgAWUJgAAAAsoTQAAABZQmgAAACygNAEAAFhAaQIAALCA0gQAAGABpQmAX2uckmF3BAB+gtIEAABgAaUJAADAAkoTAACABZQmAAAACyhNAAAAFlCaAAAALKA0AfBJOTk5GjFihKpXr67w8HC1adNGmzZtsjsWgABGaQLgkwYPHqxly5bpgw8+0I4dO9SlSxd17txZ33//vd3RAAQoShMAn3Px4kX94x//0KRJk9S+fXvVrl1bY8aMUe3atZWWllbgNi6XS9nZ2R4LANxKlCYAPufq1avKzc1VWFiYx/rw8HCtXbu2wG1SU1MVHR3tXpxOpzeiAggglCYAPicyMlKtW7fWuHHjdPz4ceXm5mru3Llav369fvjhhwK3SU5OVlZWlnvJzMz0cmoA/o7SBMAnffDBBzLG6M4775TD4dD//M//qF+/fipVquBfWw6HQ1FRUR4LANxKlCYAPik+Pl6rV6/WuXPnlJmZqY0bN+rKlSuqVauW3dEABChKEwCfFhERodjYWJ05c0YZGRnq0aOH3ZEABKgQuwMAQEEyMjJkjFG9evW0f/9+jRo1SvXr19egQYPsjgYgQDHTBMAnZWVlKSkpSfXr19eAAQPUrl07ZWRkqHTp0nZHAxCgmGkC4JN69+6t3r172x0DANyYaQIAALCA0gQAAGABpQmAX9s5NsHuCAD8BKUJAADAAkoTAACABXx6Dn4r+cA3lsZ1DM8rckzTN4Za2lfsG+ssjQMA3H6YaQIAALCAmSYAfq1xSoZKOcrcsv0dntjtlu0LwO2FmSYAAAALKE0AAAAWUJoAAAAsoDQBAABYQGkCAACwgNIEwOfk5ubqpZdeUs2aNRUeHq74+HiNGzdOxhi7owEIYNxyAIDPee2115SWlqY5c+aoUaNG2rx5swYNGqTo6GgNHz7c7ngAAhSlCX7rsgm2NO6KuVLkmAXDX7e0r77nny9yTMV311vaVyBbt26devTooW7dfr4nUo0aNfThhx9q48aNNicDEMg4PQfA57Rp00YrVqzQvn37JElff/211q5dq65duxa6jcvlUnZ2tscCALcSM00AfM7o0aOVnZ2t+vXrKzg4WLm5uRo/frz69+9f6DapqakaO3asF1MCCDTMNAHwOR9//LHS09M1b948bd26VXPmzNHkyZM1Z86cQrdJTk5WVlaWe8nMzPRiYgCBgJkmAD5n1KhRGj16tPr27StJatKkiY4cOaLU1FQlJiYWuI3D4ZDD4fBmTAABhpkmAD7nwoULKlXK89dTcHCw8vLybEoEAMw0AfBB3bt31/jx41WtWjU1atRI27Zt05QpU/T444/bHQ1AAKM0AfA506ZN00svvaShQ4fq1KlTiouL05NPPqmXX37Z7mgAAhilCYDPiYyM1NSpUzV16lS7owCAG9c0AQAAWMBME3xKSGyVIsccf7ecpX21C7N69+ii7xxeMyTM0p7O3JVb5JiKlvYEAPA1lCYAfm3n2ARFRUXZHQOAH+D0HAAAgAWUJgAAAAsoTQAAABZQmgAAACygNAEAAFhAaQIAALCA0gQAAGAB92mCTzk1o+j76Wy8O93i3oq+aaVVW1zWxjk/v2UvCQDwMcw0AQAAWEBpAgAAsIDSBMDn1KhRQ0FBQfmWpKQku6MBCGBc0wTA52zatEm5uf/98uOdO3fqgQceUK9evWxMBSDQUZoA+JyKFSt6PJ44caLi4+PVoUMHmxIBAKUJgI+7fPmy5s6dq5EjRyooKKjQcS6XSy7Xfz/mmJ2d7Y14AAII1zQB8GmLFi3S2bNnNXDgwOuOS01NVXR0tHtxOp3eCQggYFCaAPi0mTNnqmvXroqLi7vuuOTkZGVlZbmXzMxMLyUEECg4PQfAZx05ckTLly/Xp59+WuRYh8Mhh8PhhVQAAhWlCV4R3KCOpXGfN5tlYVTYzYUphudHDbU0LmLxVyWcJLDMmjVLlSpVUrdu3eyOAgCcngPgm/Ly8jRr1iwlJiYqJIT/vwNgP0oTAJ+0fPlyHT16VI8//rjdUQBAEqfnAPioLl26yBhjdwwAcGOmCQAAwAJKEwAAgAWUJgAAAAsoTQAAABZQmgAAACzg03O4KaZ1M0vj2r23wdK46FLev3GlFWWOX7I7AgDAZsw0AQAAWEBpAgAAsIDSBMCvNU7JsDsCAD9BaQIAALCA0gQAAGABpQkAAMACShMAAIAFlCYAAAALKE0AfNL333+vP/zhD4qJiVF4eLiaNGmizZs32x0LQADjjuAoVKmIiCLH9Jv1maV99Ys8ebNxblhWnrW7eP/mX88WOab+zt2W9pVnaRSKcubMGbVt21adOnXS559/rooVK+q7775T+fLl7Y4GIIBRmgD4nNdee01Op1OzZs1yr6tZs+Z1t3G5XHK5XO7H2dnZJZYPQGDi9BwAn/PPf/5TLVq0UK9evVSpUiXdfffdmjFjxnW3SU1NVXR0tHtxOp1eSgsgUFCaAPicgwcPKi0tTXXq1FFGRoaefvppDR8+XHPmzCl0m+TkZGVlZbmXzMxMLyYGEAg4PQfA5+Tl5alFixaaMGGCJOnuu+/Wzp079e677yoxMbHAbRwOhxwOhzdjAggwzDQB8DmxsbFq2LChx7oGDRro6NGjNiUCAEoTAB/Utm1b7d2712Pdvn37VL16dZsSAQClCYAPevbZZ7VhwwZNmDBB+/fv17x58/T+++8rKSnJ7mgAAhilCYDP+c1vfqOFCxfqww8/VOPGjTVu3DhNnTpV/fv3tzsagADGheAAfNJDDz2khx56yO4YAOBGaULhShU9EWnHnb6tarXgOUvj6j67ocgx3OkbAMDpOQAAAAsoTQD82s6xCXZHAOAnKE0AAAAWUJoAAAAsoDQBAABYQGkCAACwgNIEwK81TsmwOwIAP0FpAgAAsICbWwagoNKhlsbtmVzfwqhVN5WlJNWfdsLSuKslnAMA4B+YaQIAALCA0gQAAGABpQkAAMACShMAnzNmzBgFBQV5LPXrW7nGDgBKDheCA/BJjRo10vLly92PQ0L4dQXAXvwWAuCTQkJCVKVKFbtjAIAbp+cA+KTvvvtOcXFxqlWrlvr376+jR49ed7zL5VJ2drbHAgC3EqUJgM9p1aqVZs+eraVLlyotLU2HDh3Sfffdp5ycnEK3SU1NVXR0tHtxOp1eTAwgEFCaAPicrl27qlevXmratKkSEhK0ZMkSnT17Vh9//HGh2yQnJysrK8u9ZGZmejExgEDANU1+JjgqqsgxB2bUsLSvfe3evck0JeMvJ1tYG3jhYskGgdeUK1dOdevW1f79+wsd43A45HA4vJgKQKBhpgmAzzt37pwOHDig2NhYu6MACGCUJgA+5/nnn9fq1at1+PBhrVu3To888oiCg4PVr18/u6MBCGCcngPgc44dO6Z+/frp9OnTqlixotq1a6cNGzaoYsWKdkcDEMAoTQB8zvz58+2OAAD5cHoOAADAAkoTAACABZQmAH5t59gEuyMA8BOUJgAAAAu4ENzfVK5Q5JCd7WZ5IUjJWfJJa0vjqp5YV8JJAACBhJkmAAAACyhNAAAAFlCaAAAALKA0AQAAWEBpAgAAsIDSBAAAYAGlCQAAwAJKEwAAgAWUJgA+b+LEiQoKCtKIESPsjgIggHFHcD9Ta973dke4KXUXP13kmHqvb7S0L3OzYeATNm3apPfee09Nmza1OwqAAMdMEwCfde7cOfXv318zZsxQ+fLl7Y4DIMBRmgD4rKSkJHXr1k2dO3cucqzL5VJ2drbHAgC3EqfnAPik+fPna+vWrdq0aZOl8ampqRo7dmwJpwIQyJhpAuBzMjMz9cwzzyg9PV1hYWGWtklOTlZWVpZ7yczMLOGUAAINM00AfM6WLVt06tQp3XPPPe51ubm5WrNmjd5++225XC4FBwd7bONwOORwOLwdFUAAoTQB8Dm//e1vtWPHDo91gwYNUv369fXCCy/kK0wA4A2UJgA+JzIyUo0bN/ZYFxERoZiYmHzrAcBbuKYJAADAAmaaANwWVq1aZXcEAAGO0nSbyH70Xkvj+sW8U8JJSlb9Ud8WOSbv6lUvJAEAwBOn5wAAACygNAEAAFhAaQIAALCA0gQAAGABpQkAAMACShMAAIAFlCYAAAALuE8TAL/WOCVDpRxlCnzu8MRuXk4D4HZGabLZ2QGtLY37YNxkS+NqhoTdTJwSc/e0P1kad+eF9SWcBACA4uH0HAAAgAWUJgAAAAsoTQAAABZQmgAAACygNAHwOWlpaWratKmioqIUFRWl1q1b6/PPP7c7FoAAR2kC4HOqVq2qiRMnasuWLdq8ebPuv/9+9ejRQ99++63d0QAEMG45AMDndO/e3ePx+PHjlZaWpg0bNqhRo0YFbuNyueRyudyPs7OzSzQjgMDDTBMAn5abm6v58+fr/Pnzat268PuapaamKjo62r04nU4vpgQQCChNAHzSjh07VLZsWTkcDj311FNauHChGjZsWOj45ORkZWVluZfMzEwvpgUQCDg9Z7Okv3xiaZwdd/o+l+cqckzLec9Z2letVeesvagx1sbB79WrV0/bt29XVlaWFixYoMTERK1evbrQ4uRwOORwOLycEkAgoTQB8EmhoaGqXbu2JKl58+batGmT3nrrLb333ns2JwMQqDg9B+C2kJeX53GhNwB4GzNNAHxOcnKyunbtqmrVqiknJ0fz5s3TqlWrlJGRYXc0AAGM0gTA55w6dUoDBgzQDz/8oOjoaDVt2lQZGRl64IEH7I4GIIBRmgD4nJkzZ9odAQDy4ZomAAAACyhNAAAAFnB6DoBf2zk2QVFRUXbHAOAHmGkCAACwgJmmEnRoQuHfk3XNgxGTLe7N+3cEX3Kh6O/uqvXCei8kAQDAfsw0AQAAWEBpAgAAsIDTcwD8WuOUDJVylLml+zw8sdst3R+A2wMzTQAAABZQmgAAACygNAEAAFhAaQIAALCA0gQAAGABn54rhuC68ZbG3dvp2yLHRJfy/k0rX/2pqaVxXz1+l4VRRR8jcKNSU1P16aefas+ePQoPD1ebNm302muvqV69enZHAxDAmGkC4HNWr16tpKQkbdiwQcuWLdOVK1fUpUsXnT9/3u5oAAIYM00AfM7SpUs9Hs+ePVuVKlXSli1b1L59e5tSAQh0lCYAPi8rK0uSdMcddxQ6xuVyyeVyuR9nZ2eXeC4AgYXTcwB8Wl5enkaMGKG2bduqcePGhY5LTU1VdHS0e3E6i/7CaQC4EZQmAD4tKSlJO3fu1Pz58687Ljk5WVlZWe4lMzPTSwkBBApOzwHwWcOGDdO///1vrVmzRlWrVr3uWIfDIYfD4aVkAAIRpQmAzzHG6E9/+pMWLlyoVatWqWbNmnZHAgBKEwDfk5SUpHnz5mnx4sWKjIzUiRMnJEnR0dEKDw+3OR2AQMU1TQB8TlpamrKystSxY0fFxsa6l48++sjuaAACGDNNxXCuQYylcYurfVzCSfLb4ip6zMYB1u4Ibr7mbt+whzHG7ggAkA8zTQAAABZQmgAAACygNAEAAFjANU0A/NrOsQmKioqyOwYAP8BMEwAAgAWUJgAAAAsoTQAAABZwTRMAv9Y4JUOlHGXsjgHcMocndrM7QsCiNPmZx7cmFjnG+fVOLyQBAMC/cHoOAADAAkoTAACABZQmAAAACyhNAAAAFlCaAAAALKA0AfBJa9asUffu3RUXF6egoCAtWrTI7kgAAhylCYBPOn/+vJo1a6bp06fbHQUAJHGfJgA+qmvXruratavl8S6XSy6Xy/04Ozu7JGIBCGDMNAHwC6mpqYqOjnYvTqfT7kgA/AwzTbeJe7f2szSuRtKpIsfk3mwYwAclJydr5MiR7sfZ2dkUJwC3FKUJgF9wOBxyOBx2xwDgxzg9BwAAYAGlCQAAwAJOzwHwSefOndP+/fvdjw8dOqTt27frjjvuULVq1WxMBiBQUZoA+KTNmzerU6dO7sfXLvJOTEzU7NmzbUoFIJBRmgD4pI4dO8oYY3cMAHDjmiYAAAALKE0AAAAWcHoOgF/bOTZBUVFRdscA4AcoTcUQvnijpXEPLW5+y16zgvZZGsfdvgEAKBmcngMAALCA0gQAAGABpQkAAMACShMAAIAFlCYAAAALKE0AAAAWUJoAAAAsoDQBAABYQGkCAACwgNIEwGdNnz5dNWrUUFhYmFq1aqWNG63djR8ASgKlCYBP+uijjzRy5EilpKRo69atatasmRISEnTq1Cm7owEIUJQmAD5pypQpeuKJJzRo0CA1bNhQ7777rsqUKaO//e1vdkcDEKAoTQB8zuXLl7VlyxZ17tzZva5UqVLq3Lmz1q9fX+A2LpdL2dnZHgsA3EqUJgA+56efflJubq4qV67ssb5y5co6ceJEgdukpqYqOjravTidTm9EBRBAKE0A/EJycrKysrLcS2Zmpt2RAPiZELsDAMCvVahQQcHBwTp58qTH+pMnT6pKlSoFbuNwOORwOLwRD0CAYqYJgM8JDQ1V8+bNtWLFCve6vLw8rVixQq1bt7YxGYBAxkwTAJ80cuRIJSYmqkWLFmrZsqWmTp2q8+fPa9CgQXZHAxCgKE0AfFKfPn30448/6uWXX9aJEyd01113aenSpfkuDgcAb6E0AfBZw4YN07Bhw+yOAQCSuKYJAADAEkoTAACABZQmAAAACyhNAAAAFlCaAAAALKA0AQAAWEBpAgAAsIDSBAAAYAGlCQAAwAJKEwAAgAWUJgAAAAsoTQAAABZQmgAAACwIsTsAAJQEY4wkKTs72+YkAHzdtd8T135vFIbSBMAvnT59WpLkdDptTgLgdpGTk6Po6OhCn6c0AfBLd9xxhyTp6NGj1/0l6Muys7PldDqVmZmpqKgou+PcsNs9v8Qx+IqSPgZjjHJychQXF3fdcZQmAH6pVKmfL9mMjo6+bf9QXBMVFXVbH8Ptnl/iGHxFSR6Dlf+5slyaluV9clNhAAAAbmd8eg4AAMACShMAv+RwOJSSkiKHw2F3lGK73Y/hds8vcQy+wleOIcgU9fk6AAAAMNMEAABgBaUJAADAAkoTAACABZQmAAAACyhNAAAAFlCaANy2pk+frho1aigsLEytWrXSxo0brzv+k08+Uf369RUWFqYmTZpoyZIlXkpasBvJP2PGDN13330qX768ypcvr86dOxd5vN5woz+Da+bPn6+goCD9/ve/L9mAFtzoMZw9e1ZJSUmKjY2Vw+FQ3bp1b6v/liRp6tSpqlevnsLDw+V0OvXss8/q0qVLXkrrac2aNerevbvi4uIUFBSkRYsWFbnNqlWrdM8998jhcKh27dqaPXt2ieeUJBkAuA3Nnz/fhIaGmr/97W/m22+/NU888YQpV66cOXnyZIHjv/zySxMcHGwmTZpkdu3aZf7617+a0qVLmx07dng5+c9uNP+jjz5qpk+fbrZt22Z2795tBg4caKKjo82xY8e8nPy/bvQYrjl06JC58847zX333Wd69OjhnbCFuNFjcLlcpkWLFubBBx80a9euNYcOHTKrVq0y27dv93Ly/7rRY0hPTzcOh8Okp6ebQ4cOmYyMDBMbG2ueffZZLyf/2ZIlS8yLL75oPv30UyPJLFy48LrjDx48aMqUKWNGjhxpdu3aZaZNm2aCg4PN0qVLSzwrpQnAbally5YmKSnJ/Tg3N9fExcWZ1NTUAsf37t3bdOvWzWNdq1atzJNPPlmiOQtzo/l/7erVqyYyMtLMmTOnpCIWqTjHcPXqVdOmTRvzv//7vyYxMdH20nSjx5CWlmZq1aplLl++7K2IRbrRY0hKSjL333+/x7qRI0eatm3blmhOK6yUpj//+c+mUaNGHuv69OljEhISSjDZzzg9B+C2c/nyZW3ZskWdO3d2rytVqpQ6d+6s9evXF7jN+vXrPcZLUkJCQqHjS1Jx8v/ahQsXdOXKFd1xxx0lFfO6insMr7zyiipVqqQ//vGP3oh5XcU5hn/+859q3bq1kpKSVLlyZTVu3FgTJkxQbm6ut2J7KM4xtGnTRlu2bHGfwjt48KCWLFmiBx980CuZb5ad/5Ytf2EvAPiKn376Sbm5uapcubLH+sqVK2vPnj0FbnPixIkCx584caLEchamOPl/7YUXXlBcXFy+Px7eUpxjWLt2rWbOnKnt27d7IWHRinMMBw8e1H/+8x/1799fS5Ys0f79+zV06FBduXJFKSkp3ojtoTjH8Oijj+qnn35Su3btZIzR1atX9dRTT+kvf/mLNyLftML+LWdnZ+vixYsKDw8vsddmpgkAbjMTJ07U/PnztXDhQoWFhdkdx5KcnBw99thjmjFjhipUqGB3nGLLy8tTpUqV9P7776t58+bq06ePXnzxRb377rt2R7Ns1apVmjBhgt555x1t3bpVn376qT777DONGzfO7mg+j5kmALedChUqKDg4WCdPnvRYf/LkSVWpUqXAbapUqXJD40tScfJfM3nyZE2cOFHLly9X06ZNSzLmdd3oMRw4cECHDx9W9+7d3evy8vIkSSEhIdq7d6/i4+NLNvSvFOfnEBsbq9KlSys4ONi9rkGDBjpx4oQuX76s0NDQEs38a8U5hpdeekmPPfaYBg8eLElq0qSJzp8/ryFDhujFF19UqVK+PZ9S2L/lqKioEp1lkphpAnAbCg0NVfPmzbVixQr3ury8PK1YsUKtW7cucJvWrVt7jJekZcuWFTq+JBUnvyRNmjRJ48aN09KlS9WiRQtvRC3UjR5D/fr1tWPHDm3fvt29PPzww+rUqZO2b98up9PpzfiSivdzaNu2rfbv3+8ufJK0b98+xcbGer0wScU7hgsXLuQrRtdKoDGm5MLeIrb+Wy7xS80BoATMnz/fOBwOM3v2bLNr1y4zZMgQU65cOXPixAljjDGPPfaYGT16tHv8l19+aUJCQszkyZPN7t27TUpKiu23HLiR/BMnTjShoaFmwYIF5ocffnAvOTk5tuQ35saP4dd84dNzN3oMR48eNZGRkWbYsGFm79695t///repVKmSefXVV+06hBs+hpSUFBMZGWk+/PBDc/DgQfPFF1+Y+Ph407t3b1vy5+TkmG3btplt27YZSWbKlClm27Zt5siRI8YYY0aPHm0ee+wx9/hrtxwYNWqU2b17t5k+fTq3HACAokybNs1Uq1bNhIaGmpYtW5oNGza4n+vQoYNJTEz0GP/xxx+bunXrmtDQUNOoUSPz2WefeTmxpxvJX716dSMp35KSkuL94L9woz+DX/KF0mTMjR/DunXrTKtWrYzD4TC1atUy48ePN1evXvVyak83cgxXrlwxY8aMMfHx8SYsLMw4nU4zdOhQc+bMGe8HN8asXLmywP+2r2VOTEw0HTp0yLfNXXfdZUJDQ02tWrXMrFmzvJI1yJjbYC4OAADAZlzTBAAAYAGlCQAAwAJKEwAAgAWUJgAAAAsoTQAAABZQmgAAACygNAEAAFhAaQIAALCA0gQAAGABpQkAAMACShMAAIAF/w/qR/jQIYYZKAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST with CrossEntropyLoss"
      ],
      "metadata": {
        "id": "6X3RI0kvm5pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "j_bFQBhEm83F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss() #nn.NLLLoss() #\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=0.003)\n",
        "#optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1.0)\n"
      ],
      "metadata": {
        "id": "5QQrtsNhm_kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "\n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  running_loss = 0\n",
        "\n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # Get inputs\n",
        "    inputs, targets = data\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    minibatch_loss = loss.item()\n",
        "\n",
        "    running_loss += minibatch_loss\n",
        "    if i % 500 == 499:\n",
        "        print('Loss after mini-batch %5d: %.5f' %\n",
        "              (i + 1, minibatch_loss))\n",
        "        current_loss = 0.0\n",
        "\n",
        "  print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_hY2C1LnNDs",
        "outputId": "bd8cb859-e6a8-4b1e-8eff-c52caa07c7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 2.25046\n",
            "Loss after mini-batch  1000: 2.16376\n",
            "Loss after mini-batch  1500: 1.96465\n",
            "Loss after mini-batch  2000: 1.58757\n",
            "Loss after mini-batch  2500: 1.53447\n",
            "Loss after mini-batch  3000: 0.80353\n",
            "Loss after mini-batch  3500: 0.95084\n",
            "Loss after mini-batch  4000: 0.62841\n",
            "Loss after mini-batch  4500: 0.75914\n",
            "Loss after mini-batch  5000: 0.30831\n",
            "Loss after mini-batch  5500: 0.72984\n",
            "Loss after mini-batch  6000: 0.36736\n",
            "Training loss: 1.1904746963642538\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 0.53681\n",
            "Loss after mini-batch  1000: 1.08308\n",
            "Loss after mini-batch  1500: 0.17222\n",
            "Loss after mini-batch  2000: 0.32273\n",
            "Loss after mini-batch  2500: 0.68576\n",
            "Loss after mini-batch  3000: 0.49540\n",
            "Loss after mini-batch  3500: 0.89996\n",
            "Loss after mini-batch  4000: 0.72107\n",
            "Loss after mini-batch  4500: 0.08715\n",
            "Loss after mini-batch  5000: 1.15909\n",
            "Loss after mini-batch  5500: 0.16427\n",
            "Loss after mini-batch  6000: 0.25617\n",
            "Training loss: 0.3868188751336808\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 0.34473\n",
            "Loss after mini-batch  1000: 0.20918\n",
            "Loss after mini-batch  1500: 0.49215\n",
            "Loss after mini-batch  2000: 0.19498\n",
            "Loss after mini-batch  2500: 0.31197\n",
            "Loss after mini-batch  3000: 0.15188\n",
            "Loss after mini-batch  3500: 0.14149\n",
            "Loss after mini-batch  4000: 0.53511\n",
            "Loss after mini-batch  4500: 0.39472\n",
            "Loss after mini-batch  5000: 0.21145\n",
            "Loss after mini-batch  5500: 0.71297\n",
            "Loss after mini-batch  6000: 0.06088\n",
            "Training loss: 0.31486154966397834\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 0.16839\n",
            "Loss after mini-batch  1000: 0.13464\n",
            "Loss after mini-batch  1500: 0.06833\n",
            "Loss after mini-batch  2000: 0.56926\n",
            "Loss after mini-batch  2500: 0.19176\n",
            "Loss after mini-batch  3000: 0.34995\n",
            "Loss after mini-batch  3500: 0.43087\n",
            "Loss after mini-batch  4000: 0.06487\n",
            "Loss after mini-batch  4500: 0.24522\n",
            "Loss after mini-batch  5000: 0.31683\n",
            "Loss after mini-batch  5500: 0.34262\n",
            "Loss after mini-batch  6000: 0.03751\n",
            "Training loss: 0.27681184728986896\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 0.32519\n",
            "Loss after mini-batch  1000: 0.86186\n",
            "Loss after mini-batch  1500: 0.06744\n",
            "Loss after mini-batch  2000: 0.81044\n",
            "Loss after mini-batch  2500: 0.04933\n",
            "Loss after mini-batch  3000: 0.30404\n",
            "Loss after mini-batch  3500: 0.15972\n",
            "Loss after mini-batch  4000: 0.33133\n",
            "Loss after mini-batch  4500: 0.03876\n",
            "Loss after mini-batch  5000: 0.42773\n",
            "Loss after mini-batch  5500: 0.17214\n",
            "Loss after mini-batch  6000: 0.34788\n",
            "Training loss: 0.2477605757758332\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "img = images[4].view(1, 784)\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mkKHtsZoSLg",
        "outputId": "e0241f44-6fb6-47b5-a2e2-e41ed6d53b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn off gradients to speed up this part\n",
        "with torch.inference_mode():#torch.no_grad():\n",
        "    logps = mlp(img)"
      ],
      "metadata": {
        "id": "a-z2mL2GoVxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksU53DWwoWlF",
        "outputId": "d0800f70-730c-4361-9258-cc2bc8df1aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.5915, -8.2068,  3.8632,  1.4661, -6.0676,  4.5186, -3.6643, -1.8895,\n",
              "          3.2487, -1.4855]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = torch.nn.functional.softmax(logps, dim=1)\n",
        "ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HaaN6nYokly",
        "outputId": "4ab48ce2-3680-473c-f1b2-192df9c8d093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.8853e-01, 1.8419e-08, 3.2155e-03, 2.9252e-04, 1.5644e-07, 6.1927e-03,\n",
              "         1.7300e-06, 1.0206e-05, 1.7392e-03, 1.5287e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "GoVdCJ86ouBU",
        "outputId": "fcc46634-bf32-456e-cd69-c39a266e12b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x900 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgqCeC19Y6Vl",
        "outputId": "56afc10d-2a91-4ab7-92a6-b4de462c5113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9.2253e-06, -2.2464e-06, -1.8543e-05,  4.4214e-06,  1.6578e-05,\n",
              "          1.1587e-05,  1.9855e-05, -1.0638e-05, -4.3036e-06,  1.2927e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}