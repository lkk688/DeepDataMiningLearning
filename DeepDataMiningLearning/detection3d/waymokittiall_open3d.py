#!/usr/bin/env python3
"""
Optimized and reorganized KITTI and Waymo2KITTI dataset visualization using Open3D.
Replaces Mayavi with Open3D for colorful 3D point cloud and 3D bounding box visualization.
Supports headless environment with PLY file export.

This code is based on the original waymokittiall.py and maintains full compatibility
with the KITTI and Waymo2KITTI dataset formats generated by waymo2kitti.py.

Mathematical Foundations:
========================

1. 3D Bounding Box Corner Generation:
   For a box with center (x,y,z), dimensions (dx,dy,dz), and heading θ:
   
   Template corners (before rotation):
   $$\mathbf{T} = \frac{1}{2} \begin{bmatrix}
   1 & 1 & -1 \\
   1 & -1 & -1 \\
   -1 & -1 & -1 \\
   -1 & 1 & -1 \\
   1 & 1 & 1 \\
   1 & -1 & 1 \\
   -1 & -1 & 1 \\
   -1 & 1 & 1
   \end{bmatrix}$$
   
   Scaled corners: $\mathbf{C}_s = \mathbf{T} \odot [dx, dy, dz]$
   
   Rotation matrix around Z-axis:
   $$\mathbf{R}_z(\theta) = \begin{bmatrix}
   \cos\theta & -\sin\theta & 0 \\
   \sin\theta & \cos\theta & 0 \\
   0 & 0 & 1
   \end{bmatrix}$$
   
   Final corners: $\mathbf{C} = \mathbf{C}_s \mathbf{R}_z(\theta) + [x, y, z]$

2. Point Cloud Intensity-Based Coloring:
   For intensity values I ∈ [I_min, I_max]:
   
   Normalized intensity: $I_n = \frac{I - I_{min}}{I_{max} - I_{min}}$
   
   RGB mapping using viridis colormap:
   $$\begin{align}
   R &= 0.267004 + I_n(-0.127568 + I_n(0.472873 + I_n(-0.498882))) \\
   G &= 0.004874 + I_n(1.424006 + I_n(-2.174996 + I_n(1.074935))) \\
   B &= 0.329415 + I_n(0.226420 + I_n(-0.020582 + I_n(-0.266134)))
   \end{align}$$

3. Coordinate System Transformations:
   KITTI uses right-handed coordinate system:
   - X: forward (vehicle direction)
   - Y: left
   - Z: up
   
   Waymo2KITTI maintains KITTI coordinate conventions after conversion.

"""

import numpy as np
import os
import cv2
import sys
import argparse
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.colors as mcolors
import math
import torch
from typing import List, Tuple, Optional, Union
import warnings

# Calibration utilities will be implemented directly in this file

# Suppress Open3D warnings in headless mode
warnings.filterwarnings("ignore", category=UserWarning)

try:
    import open3d as o3d
    OPEN3D_AVAILABLE = True
except ImportError:
    print("Warning: Open3D not available. Install with: pip install open3d")
    OPEN3D_AVAILABLE = False

# Configuration
plt.rcParams['figure.figsize'] = (16, 9)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
sys.path.append(ROOT_DIR)

# Color mappings for different object types
box_colormap = [
    [1, 1, 1],      # White
    [0, 1, 0],      # Green
    [0, 1, 1],      # Cyan
    [1, 1, 0],      # Yellow
]

INSTANCE_Color = {
    'Car': 'red', 
    'Pedestrian': 'green', 
    'Sign': 'yellow', 
    'Cyclist': 'purple'
}

INSTANCE3D_Color = {
    'Car': (0, 1, 0),           # Green
    'Pedestrian': (0, 1, 1),   # Cyan
    'Sign': (1, 1, 0),         # Yellow
    'Cyclist': (0.5, 0.5, 0.3) # Brown
}

INSTANCE3D_ColorCV2 = {
    'Car': (0, 255, 0), 
    'Pedestrian': (255, 255, 0), 
    'Sign': (0, 255, 255), 
    'Cyclist': (127, 127, 64)
}

waymocameraorder = {
    0: 1, 1: 0, 2: 2, 3: 3, 4: 4
}  # Front, front_left, side_left, front_right, side_right

cameraname_map = {
    0: "FRONT", 
    1: "FRONT_LEFT", 
    2: "FRONT_RIGHT", 
    3: "SIDE_LEFT", 
    4: "SIDE_RIGHT"
}


def check_numpy_to_torch(x: Union[np.ndarray, torch.Tensor]) -> Tuple[torch.Tensor, bool]:
    """
    Convert numpy array to torch tensor if needed.
    
    Args:
        x: Input array or tensor
        
    Returns:
        Tuple of (tensor, was_numpy_flag)
    """
    if isinstance(x, np.ndarray):
        return torch.from_numpy(x).float(), True
    return x, False


def rotate_points_along_z(points: Union[np.ndarray, torch.Tensor], 
                         angle: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:
    """
    Rotate points around Z-axis using rotation matrix.
    
    Mathematical Formula:
    $$\mathbf{R}_z(\theta) = \begin{bmatrix}
    \cos\theta & -\sin\theta & 0 \\
    \sin\theta & \cos\theta & 0 \\
    0 & 0 & 1
    \end{bmatrix}$$
    
    Args:
        points: (B, N, 3 + C) points to rotate
        angle: (B,) rotation angles in radians
        
    Returns:
        Rotated points with same shape as input
    """
    points, is_numpy = check_numpy_to_torch(points)
    angle, _ = check_numpy_to_torch(angle)

    cosa = torch.cos(angle)
    sina = torch.sin(angle)
    zeros = angle.new_zeros(points.shape[0])
    ones = angle.new_ones(points.shape[0])
    
    # Construct rotation matrix: R_z(θ)
    rot_matrix = torch.stack((
        cosa,  sina, zeros,
        -sina, cosa, zeros,
        zeros, zeros, ones
    ), dim=1).view(-1, 3, 3).float()
    
    # Apply rotation to XYZ coordinates only
    points_rot = torch.matmul(points[:, :, 0:3], rot_matrix)
    points_rot = torch.cat((points_rot, points[:, :, 3:]), dim=-1)
    
    return points_rot.numpy() if is_numpy else points_rot


def boxes_to_corners_3d(boxes3d: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:
    """
    Generate 8 corner points for 3D bounding boxes.
    
    Corner ordering (KITTI format):
        7 -------- 4
       /|         /|
      6 -------- 5 .
      | |        | |
      . 3 -------- 0
      |/         |/
      2 -------- 1
    
    Mathematical Process:
    1. Create template corners: T = [±1, ±1, ±1] / 2
    2. Scale by box dimensions: C_s = T ⊙ [dx, dy, dz]
    3. Rotate around Z-axis: C_r = C_s * R_z(θ)
    4. Translate to box center: C = C_r + [x, y, z]
    
    Args:
        boxes3d: (N, 7) [x, y, z, dx, dy, dz, heading]
        
    Returns:
        corners3d: (N, 8, 3) corner coordinates
    """
    boxes3d, is_numpy = check_numpy_to_torch(boxes3d)

    # Template for 8 corners of a unit cube
    template = boxes3d.new_tensor((
        [1, 1, -1], [1, -1, -1], [-1, -1, -1], [-1, 1, -1],  # Bottom face
        [1, 1, 1], [1, -1, 1], [-1, -1, 1], [-1, 1, 1],      # Top face
    )) / 2

    # Scale template by box dimensions
    corners3d = boxes3d[:, None, 3:6].repeat(1, 8, 1) * template[None, :, :]
    
    # Rotate corners around Z-axis by heading angle
    corners3d = rotate_points_along_z(corners3d.view(-1, 8, 3), boxes3d[:, 6]).view(-1, 8, 3)
    
    # Translate to box center
    corners3d += boxes3d[:, None, 0:3]

    return corners3d.numpy() if is_numpy else corners3d


class Open3DVisualizer:
    """
    Open3D-based 3D visualization system for KITTI and Waymo2KITTI datasets.
    
    This class provides comprehensive 3D visualization capabilities including:
    - Colorful point cloud rendering with intensity-based coloring
    - 3D bounding box visualization with class-specific colors
    - Coordinate system axes and ground grid
    - Headless rendering with PLY export support
    """
    
    def __init__(self, headless: bool = False, window_size: Tuple[int, int] = (1024, 768)):
        """
        Initialize the Open3D visualizer.
        
        Args:
            headless: Whether to run in headless mode (no GUI)
            window_size: Window dimensions for GUI mode
        """
        self.headless = headless
        self.window_size = window_size
        self.vis = None
        self.geometries = []
        
        if not OPEN3D_AVAILABLE:
            raise ImportError("Open3D is required but not installed. Run: pip install open3d")
    
    def create_point_cloud(self, points: np.ndarray, 
                          color_by: str = 'intensity',
                          point_size: float = 2.0) -> o3d.geometry.PointCloud:
        """
        Create Open3D point cloud from numpy array.
        
        Supports both KITTI format (N, 4) and Waymo2KITTI format (N, 6).
        Waymo2KITTI format: [x, y, z, intensity, elongation, mask_indices]
        
        Coloring options:
        - 'intensity': Color by intensity values (column 3)
        - 'elongation': Color by elongation values (column 4, Waymo2KITTI only)
        - 'mask_indices': Color by mask indices (column 5, Waymo2KITTI only)
        - 'height': Color by Z coordinate (height)
        - 'none': Use default white color
        
        Intensity-based coloring uses viridis colormap approximation:
        $$\text{RGB} = f_{\text{viridis}}(\frac{I - I_{\min}}{I_{\max} - I_{\min}})$$
        
        Args:
            points: (N, 3), (N, 4), or (N, 6) array with XYZ and optional features
            color_by: Coloring method ('intensity', 'elongation', 'mask_indices', 'height', 'none')
            point_size: Point rendering size
            
        Returns:
            Open3D PointCloud object
        """
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points[:, :3])
        
        # Determine coloring
        if color_by == 'intensity' and points.shape[1] >= 4:
            values = points[:, 3]
            colors = self._apply_colormap(values, 'viridis')
        elif color_by == 'elongation' and points.shape[1] >= 5:
            values = points[:, 4]
            colors = self._apply_colormap(values, 'plasma')
        elif color_by == 'mask_indices' and points.shape[1] >= 6:
            values = points[:, 5]
            colors = self._apply_discrete_colormap(values)
        elif color_by == 'height':
            values = points[:, 2]  # Z coordinate
            colors = self._apply_colormap(values, 'coolwarm')
        else:
            # Default white color
            colors = np.ones((len(points), 3)) * 0.8
        
        pcd.colors = o3d.utility.Vector3dVector(colors)
        return pcd
    
    def _apply_colormap(self, values: np.ndarray, colormap: str = 'viridis') -> np.ndarray:
        """
        Apply colormap to values with automatic normalization.
        
        Args:
            values: Raw values to be colored
            colormap: Colormap type ('viridis', 'plasma', 'coolwarm')
            
        Returns:
            RGB colors array (N, 3)
        """
        # Normalize values to [0, 1]
        if values.max() > values.min():
            normalized_values = (values - values.min()) / (values.max() - values.min())
        else:
            normalized_values = np.ones_like(values) * 0.5
        
        if colormap == 'viridis':
            return self._apply_viridis_colormap(normalized_values)
        elif colormap == 'plasma':
            return self._apply_plasma_colormap(normalized_values)
        elif colormap == 'coolwarm':
            return self._apply_coolwarm_colormap(normalized_values)
        else:
            return self._apply_viridis_colormap(normalized_values)
    
    def _apply_discrete_colormap(self, values: np.ndarray) -> np.ndarray:
        """
        Apply discrete colormap for categorical values (e.g., mask indices).
        
        Args:
            values: Discrete values (mask indices)
            
        Returns:
            RGB colors array (N, 3)
        """
        unique_values = np.unique(values)
        colors = np.zeros((len(values), 3))
        
        # Define distinct colors for different mask indices
        color_palette = [
            [1.0, 0.0, 0.0],  # Red
            [0.0, 1.0, 0.0],  # Green
            [0.0, 0.0, 1.0],  # Blue
            [1.0, 1.0, 0.0],  # Yellow
            [1.0, 0.0, 1.0],  # Magenta
            [0.0, 1.0, 1.0],  # Cyan
            [0.5, 0.5, 0.5],  # Gray
            [1.0, 0.5, 0.0],  # Orange
            [0.5, 0.0, 1.0],  # Purple
            [0.0, 0.5, 0.0],  # Dark Green
        ]
        
        for i, val in enumerate(unique_values):
            mask = values == val
            color_idx = int(val) % len(color_palette)
            colors[mask] = color_palette[color_idx]
        
        return colors

    def _apply_viridis_colormap(self, values: np.ndarray) -> np.ndarray:
        """
        Apply viridis-like colormap to normalized values [0, 1].
        
        Viridis colormap approximation:
        $$\begin{align}
        R &= 0.267004 + v(-0.127568 + v(0.472873 + v(-0.498882))) \\
        G &= 0.004874 + v(1.424006 + v(-2.174996 + v(1.074935))) \\
        B &= 0.329415 + v(0.226420 + v(-0.020582 + v(-0.266134)))
        \end{align}$$
        
        Args:
            values: Normalized values in [0, 1]
            
        Returns:
            RGB colors array (N, 3)
        """
        v = np.clip(values, 0, 1)
        
        # Viridis colormap polynomial approximation
        r = 0.267004 + v * (-0.127568 + v * (0.472873 + v * (-0.498882)))
        g = 0.004874 + v * (1.424006 + v * (-2.174996 + v * (1.074935)))
        b = 0.329415 + v * (0.226420 + v * (-0.020582 + v * (-0.266134)))
        
        return np.column_stack([r, g, b])
    
    def _apply_plasma_colormap(self, values: np.ndarray) -> np.ndarray:
        """
        Apply plasma-like colormap to normalized values [0, 1].
        
        Args:
            values: Normalized values in [0, 1]
            
        Returns:
            RGB colors array (N, 3)
        """
        v = np.clip(values, 0, 1)
        
        # Plasma colormap approximation
        r = 0.050383 + v * (2.176514 + v * (-2.689460 + v * (6.130348 + v * (-11.10743 + v * (10.02306 + v * (-3.658713))))))
        g = 0.029803 + v * (0.280267 + v * (0.753867 + v * (-2.41777 + v * (5.168399 + v * (-5.435545 + v * (2.062490))))))
        b = 0.527975 + v * (0.581122 + v * (0.253935 + v * (-6.393885 + v * (13.35155 + v * (-12.24460 + v * (4.27415))))))
        
        return np.column_stack([np.clip(r, 0, 1), np.clip(g, 0, 1), np.clip(b, 0, 1)])
    
    def _apply_coolwarm_colormap(self, values: np.ndarray) -> np.ndarray:
        """
        Apply coolwarm colormap to normalized values [0, 1].
        
        Args:
            values: Normalized values in [0, 1]
            
        Returns:
            RGB colors array (N, 3)
        """
        v = np.clip(values, 0, 1)
        
        # Cool-warm colormap: blue (0) to white (0.5) to red (1)
        r = np.where(v < 0.5, 2 * v, 1.0)
        g = np.where(v < 0.5, 2 * v, 2 * (1 - v))
        b = np.where(v < 0.5, 1.0, 2 * (1 - v))
        
        return np.column_stack([r, g, b])
    
    def create_bounding_box(self, corners: np.ndarray, 
                           color: Tuple[float, float, float] = (0, 1, 0),
                           line_width: float = 2.0) -> o3d.geometry.LineSet:
        """
        Create 3D bounding box from 8 corner points.
        
        Corner connectivity follows KITTI convention:
        - Bottom face: 0-1-2-3-0
        - Top face: 4-5-6-7-4  
        - Vertical edges: 0-4, 1-5, 2-6, 3-7
        
        Args:
            corners: (8, 3) corner coordinates
            color: RGB color tuple
            line_width: Line thickness
            
        Returns:
            Open3D LineSet object
        """
        # Define the 12 edges of a bounding box
        lines = [
            [0, 1], [1, 2], [2, 3], [3, 0],  # Bottom face
            [4, 5], [5, 6], [6, 7], [7, 4],  # Top face
            [0, 4], [1, 5], [2, 6], [3, 7],  # Vertical edges
        ]
        
        line_set = o3d.geometry.LineSet()
        line_set.points = o3d.utility.Vector3dVector(corners)
        line_set.lines = o3d.utility.Vector2iVector(lines)
        
        # Set uniform color for all lines
        colors = [color for _ in range(len(lines))]
        line_set.colors = o3d.utility.Vector3dVector(colors)
        
        return line_set
    
    def create_coordinate_frame(self, size: float = 3.0, origin: np.ndarray = None) -> o3d.geometry.TriangleMesh:
        """
        Create coordinate system axes.
        
        KITTI coordinate system:
        - X-axis (Red): Forward direction
        - Y-axis (Green): Left direction  
        - Z-axis (Blue): Up direction
        
        Args:
            size: Axis length
            origin: Origin position (default: [0, 0, 0])
            
        Returns:
            Open3D coordinate frame mesh
        """
        if origin is None:
            origin = np.array([0.0, 0.0, 0.0])
        
        coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
            size=size, origin=origin
        )
        return coord_frame
    
    def create_ground_grid(self, size: float = 100.0, step: float = 10.0, 
                          height: float = 0.0) -> o3d.geometry.LineSet:
        """
        Create ground plane grid for reference.
        
        Grid extends from -size/2 to +size/2 in X and Y directions.
        
        Args:
            size: Grid total size
            step: Grid line spacing
            height: Z-coordinate of grid plane
            
        Returns:
            Open3D LineSet for grid
        """
        lines = []
        points = []
        
        # Create grid lines parallel to X-axis
        for y in np.arange(-size/2, size/2 + step, step):
            points.extend([[-size/2, y, height], [size/2, y, height]])
            lines.append([len(points)-2, len(points)-1])
        
        # Create grid lines parallel to Y-axis  
        for x in np.arange(-size/2, size/2 + step, step):
            points.extend([[x, -size/2, height], [x, size/2, height]])
            lines.append([len(points)-2, len(points)-1])
        
        line_set = o3d.geometry.LineSet()
        line_set.points = o3d.utility.Vector3dVector(points)
        line_set.lines = o3d.utility.Vector2iVector(lines)
        
        # Gray color for grid
        colors = [(0.5, 0.5, 0.5) for _ in range(len(lines))]
        line_set.colors = o3d.utility.Vector3dVector(colors)
        
        return line_set
    
    def visualize_scene(self, points: np.ndarray, 
                       boxes: Optional[List] = None,
                       point_cloud_range: Optional[List[float]] = None,
                       color_by: str = 'intensity',
                       save_path: Optional[str] = None,
                       show_coordinate_frame: bool = True,
                       show_ground_grid: bool = True,
                       calib=None) -> None:
        """
        Visualize complete 3D scene with point cloud and bounding boxes.
        
        Args:
            points: (N, 3), (N, 4), or (N, 6) point cloud array
            boxes: List of Object3d instances or box arrays
            point_cloud_range: [xmin, ymin, zmin, xmax, ymax, zmax] for filtering
            color_by: Point cloud coloring method
            save_path: Path to save visualization (PLY format)
            show_coordinate_frame: Whether to show coordinate axes
            show_ground_grid: Whether to show ground grid
            calib: Calibration object for coordinate transformation
        """
        if not OPEN3D_AVAILABLE:
            print("Open3D not available, skipping visualization")
            return
        
        self.geometries = []
        
        # Filter points if range specified
        if point_cloud_range is not None:
            points = self._filter_points(points, point_cloud_range)
        
        # Create point cloud with specified coloring
        pcd = self.create_point_cloud(points, color_by=color_by)
        self.geometries.append(pcd)
        
        # Print LiDAR point cloud range
        print(f"LiDAR point cloud range:")
        print(f"  X=[{points[:, 0].min():.2f}, {points[:, 0].max():.2f}], Y=[{points[:, 1].min():.2f}, {points[:, 1].max():.2f}], Z=[{points[:, 2].min():.2f}, {points[:, 2].max():.2f}]")
        print(f"  Total points: {len(points)}")
        
        # Add bounding boxes
        if boxes is not None:
            for obj in boxes:
                if hasattr(obj, 'type') and obj.type == "DontCare":
                    continue
                
                # Get box color based on object type
                if hasattr(obj, 'type') and obj.type in INSTANCE3D_Color:
                    color = INSTANCE3D_Color[obj.type]
                else:
                    color = (1, 1, 1)  # Default white
                
                # Generate box corners
                if hasattr(obj, 'h'):  # Object3d format
                    corners = self._object3d_to_corners(obj)
                    
                    # Print original corners in camera coordinates
                    print(f"{obj.type} corners in camera coordinates:")
                    print(f"  Center: ({obj.t[0]:.2f}, {obj.t[1]:.2f}, {obj.t[2]:.2f})")
                    print(f"  Dimensions: L={obj.l:.2f}, W={obj.w:.2f}, H={obj.h:.2f}")
                    print(f"  Rotation: {obj.ry:.2f} rad")
                    print(f"  Corner range: X=[{corners[0].min():.2f}, {corners[0].max():.2f}], Y=[{corners[1].min():.2f}, {corners[1].max():.2f}], Z=[{corners[2].min():.2f}, {corners[2].max():.2f}]")
                    
                    # Transform corners from camera coordinates to LiDAR coordinates
                    if calib is not None:
                        try:
                            corners_velo = calib.project_rect_to_velo(corners)  # (8, 3)
                            corners = corners_velo
                        except Exception as e:
                            print(f"Warning: Failed to transform coordinates for {obj.type}: {e}")
                            # Use original corners if transformation fails
                            pass
                else:  # Assume numpy array format [x, y, z, dx, dy, dz, heading]
                    corners = boxes_to_corners_3d(np.array([obj]))[0]
                
                # Create and add bounding box
                bbox = self.create_bounding_box(corners, color)
                self.geometries.append(bbox)
        
        # Add coordinate frame
        if show_coordinate_frame:
            coord_frame = self.create_coordinate_frame(size=5.0)
            self.geometries.append(coord_frame)
        
        # Add ground grid
        if show_ground_grid:
            grid = self.create_ground_grid(size=100.0, step=10.0)
            self.geometries.append(grid)
        
        # Visualize or save
        if self.headless or save_path:
            if save_path:
                self._save_scene(save_path)
                print(f"Scene saved to: {save_path}")
        else:
            self._show_interactive()
    
    def _filter_points(self, points: np.ndarray, 
                      point_cloud_range: List[float]) -> np.ndarray:
        """
        Filter points within specified 3D range.
        
        Range format: [xmin, ymin, zmin, xmax, ymax, zmax]
        
        Args:
            points: Input point cloud
            point_cloud_range: 3D bounding range
            
        Returns:
            Filtered points
        """
        mask = (
            (points[:, 0] >= point_cloud_range[0]) & (points[:, 0] <= point_cloud_range[3]) &
            (points[:, 1] >= point_cloud_range[1]) & (points[:, 1] <= point_cloud_range[4]) &
            (points[:, 2] >= point_cloud_range[2]) & (points[:, 2] <= point_cloud_range[5])
        )
        
        if points.shape[1] >= 4:  # Has intensity
            mask = mask & (points[:, 3] <= 1.0)  # Filter invalid intensity
        
        filtered_points = points[mask]
        print(f"Filtered points: {len(points)} -> {len(filtered_points)}")
        return filtered_points
    
    def _object3d_to_corners(self, obj) -> np.ndarray:
        """
        Convert Object3d instance to 8 corner coordinates.
        
        Uses the compute_box_3d function from original code.
        
        Args:
            obj: Object3d instance
            
        Returns:
            (8, 3) corner coordinates
        """
        return compute_box_3d(obj).T  # Transpose to get (8, 3)
    
    def _save_scene(self, save_path: str) -> None:
        """
        Save scene geometries to PLY file.
        
        Args:
            save_path: Output file path
        """
        # Combine all point clouds
        combined_pcd = o3d.geometry.PointCloud()
        
        for geom in self.geometries:
            if isinstance(geom, o3d.geometry.PointCloud):
                combined_pcd += geom
            elif isinstance(geom, o3d.geometry.LineSet):
                # Convert line set to point cloud for saving
                line_pcd = o3d.geometry.PointCloud()
                line_pcd.points = geom.points
                if len(geom.colors) > 0:
                    line_pcd.colors = geom.colors
                combined_pcd += line_pcd
        
        # Save combined point cloud
        if len(combined_pcd.points) > 0:
            o3d.io.write_point_cloud(save_path, combined_pcd)
    
    def _show_interactive(self) -> None:
        """Show interactive visualization window."""
        if self.headless:
            return
        
        self.vis = o3d.visualization.Visualizer()
        self.vis.create_window(width=self.window_size[0], height=self.window_size[1])
        
        # Add all geometries
        for geom in self.geometries:
            self.vis.add_geometry(geom)
        
        # Set viewing parameters
        ctr = self.vis.get_view_control()
        ctr.set_front([0.0, 0.0, -1.0])  # Look down negative Z
        ctr.set_lookat([0.0, 0.0, 0.0])  # Look at origin
        ctr.set_up([0.0, -1.0, 0.0])     # Y-axis points up in view
        ctr.set_zoom(0.3)
        
        # Run visualization
        self.vis.run()
        self.vis.destroy_window()


# Original data loading and processing functions (maintained for compatibility)

class Object3d(object):
    """
    3D object label class for KITTI format.
    
    Parses KITTI label format:
    type truncated occluded alpha bbox_2d dimensions location rotation_y
    """

    def __init__(self, label_file_line: str):
        """
        Initialize from KITTI label line.
        
        Args:
            label_file_line: Single line from KITTI label file
        """
        data = label_file_line.split(" ")
        data[1:] = [float(x) for x in data[1:]]

        # Extract label, truncation, occlusion
        self.type = data[0]  # 'Car', 'Pedestrian', ...
        self.truncation = data[1]  # truncated pixel ratio [0..1]
        self.occlusion = int(data[2])  # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown
        self.alpha = data[3]  # object observation angle [-pi..pi]

        # Extract 2D bounding box in 0-based coordinates
        self.xmin = data[4]  # left
        self.ymin = data[5]  # top
        self.xmax = data[6]  # right
        self.ymax = data[7]  # bottom
        self.box2d = np.array([self.xmin, self.ymin, self.xmax, self.ymax])

        # Extract 3D bounding box information
        self.h = data[8]   # box height
        self.w = data[9]   # box width
        self.l = data[10]  # box length (in meters)
        self.t = (data[11], data[12], data[13])  # location (x,y,z) in camera coord.
        self.ry = data[14]  # yaw angle (around Y-axis in camera coordinates) [-pi..pi]

    def estimate_difficulty(self) -> str:
        """Estimate detection difficulty as defined in KITTI."""
        bb_height = np.abs(self.xmax - self.xmin)

        if bb_height >= 40 and self.occlusion == 0 and self.truncation <= 0.15:
            return "Easy"
        elif bb_height >= 25 and self.occlusion in [0, 1] and self.truncation <= 0.30:
            return "Moderate"
        elif bb_height >= 25 and self.occlusion in [0, 1, 2] and self.truncation <= 0.50:
            return "Hard"
        else:
            return "Unknown"

    def print_object(self):
        """Print object information."""
        print(f"Type, truncation, occlusion, alpha: {self.type}, {self.truncation}, {self.occlusion}, {self.alpha}")
        print(f"2d bbox (x0,y0,x1,y1): {self.xmin}, {self.ymin}, {self.xmax}, {self.ymax}")
        print(f"3d bbox h,w,l: {self.h}, {self.w}, {self.l}")
        print(f"3d bbox location, ry: ({self.t[0]}, {self.t[1]}, {self.t[2]}), {self.ry}")
        print(f"Difficulty of estimation: {self.estimate_difficulty()}")


def load_velo_scan(velo_filename: str, dtype=np.float32, n_vec: int = 6, 
                  filterpoints: bool = False, 
                  point_cloud_range: List[float] = [0, -15, -5, 90, 15, 4]) -> np.ndarray:
    """
    Load velodyne point cloud from binary file.
    
    Supports both legacy KITTI format (N, 4) and Waymo2KITTI format (N, 6).
    Waymo2KITTI format: [x, y, z, intensity, elongation, mask_indices]
    
    Args:
        velo_filename: Path to .bin file
        dtype: Data type for loading
        n_vec: Number of values per point (4 for KITTI: x,y,z,intensity; 6 for Waymo2KITTI: x,y,z,intensity,elongation,mask_indices)
        filterpoints: Whether to filter points by range
        point_cloud_range: [xmin, ymin, zmin, xmax, ymax, zmax]
        
    Returns:
        Point cloud array (N, n_vec)
    """
    scan = np.fromfile(velo_filename, dtype=dtype)
    scan = scan.reshape((-1, n_vec))
    
    xpoints = scan[:, 0]
    ypoints = scan[:, 1]
    zpoints = scan[:, 2]
    
    print(f"Loaded point cloud shape: {scan.shape}")
    print(f"X range: {min(xpoints):.2f} to {max(xpoints):.2f}")
    print(f"Y range: {min(ypoints):.2f} to {max(ypoints):.2f}")
    print(f"Z range: {min(zpoints):.2f} to {max(zpoints):.2f}")
    
    if n_vec >= 4:
        intensities = scan[:, 3]
        print(f"Intensity range: {min(intensities):.2f} to {max(intensities):.2f}")
    
    if n_vec >= 6:
        elongations = scan[:, 4]
        mask_indices = scan[:, 5]
        print(f"Elongation range: {min(elongations):.2f} to {max(elongations):.2f}")
        print(f"Mask indices range: {min(mask_indices):.0f} to {max(mask_indices):.0f}")
    
    if filterpoints:
        print(f"Filtering points in range: x[{point_cloud_range[0]}, {point_cloud_range[3]}], "
              f"y[{point_cloud_range[1]}, {point_cloud_range[4]}], "
              f"z[{point_cloud_range[2]}, {point_cloud_range[5]}]")
        scan = filter_lidarpoints(scan, point_cloud_range)
    
    return scan


def filter_lidarpoints(pc_velo: np.ndarray, 
                      point_cloud_range: List[float] = [0, -15, -5, 90, 15, 4]) -> np.ndarray:
    """
    Filter LiDAR points within specified range.
    
    Supports both KITTI format (N, 4) and Waymo2KITTI format (N, 6).
    
    Args:
        pc_velo: Point cloud array (N, 4) or (N, 6)
        point_cloud_range: [xmin, ymin, zmin, xmax, ymax, zmax]
        
    Returns:
        Filtered point cloud
    """
    print(f"Original points: {pc_velo.shape}")
    print(f"Filtering range: x[{point_cloud_range[0]}, {point_cloud_range[3]}], "
          f"y[{point_cloud_range[1]}, {point_cloud_range[4]}], "
          f"z[{point_cloud_range[2]}, {point_cloud_range[5]}]")
    
    # Basic spatial filtering
    spatial_mask = (
        (pc_velo[:, 0] >= point_cloud_range[0]) & (pc_velo[:, 0] <= point_cloud_range[3]) &
        (pc_velo[:, 1] >= point_cloud_range[1]) & (pc_velo[:, 1] <= point_cloud_range[4]) &
        (pc_velo[:, 2] >= point_cloud_range[2]) & (pc_velo[:, 2] <= point_cloud_range[5])
    )
    print(f"Points after spatial filtering: {np.sum(spatial_mask)}")
    
    # Start with spatial mask
    mask = spatial_mask
    
    # Add intensity filtering if available (more lenient for Waymo data)
    if pc_velo.shape[1] >= 4:
        # For Waymo data, intensity can be much higher than 1.0
        intensity_mask = (pc_velo[:, 3] >= 0) & (pc_velo[:, 3] <= 100)  # More reasonable range
        mask = mask & intensity_mask
        print(f"Points after intensity filtering: {np.sum(mask)}")
    
    # For Waymo2KITTI format, be more lenient with mask indices
    #The mask_indices list (which is one of the function's return values) contains five arrays. 
    # The array corresponding to the TOP lidar has the unique 1D index for each of its points, while the other four arrays (front, rear, left, right) just contain -1s.
    # if pc_velo.shape[1] >= 6:
    #     # Allow -1 (invalid) and reasonable positive values
    #     # The large values like 323972 might be encoding errors, so filter them out
    #     mask_indices_mask = (pc_velo[:, 5] >= -1) & (pc_velo[:, 5] < 1000)  # More lenient range
    #     mask = mask & mask_indices_mask
    #     print(f"Points after mask indices filtering: {np.sum(mask)}")
    
    filtered_points = pc_velo[mask]
    print(f"Final filtered points: {filtered_points.shape}")
    return filtered_points


def read_label(label_filename: str) -> List[Object3d]:
    """
    Read KITTI format label file.
    
    Args:
        label_filename: Path to label file
        
    Returns:
        List of Object3d instances
    """
    if os.path.exists(label_filename):
        lines = [line.rstrip() for line in open(label_filename)]
        objects = [Object3d(line) for line in lines]
        return objects
    else:
        return []


def read_multi_label(label_files: List[str]) -> List[List[Object3d]]:
    """
    Read multiple label files.
    
    Args:
        label_files: List of label file paths
        
    Returns:
        List of object lists for each file
    """
    objectlabels = []
    for label_file in label_files:
        object3dlabel = read_label(label_file)
        objectlabels.append(object3dlabel)
    return objectlabels


def load_image(img_filenames: List[str]) -> List[np.ndarray]:
    """
    Load multiple images, automatically detecting PNG or JPG format.
    
    Args:
        img_filenames: List of image file paths (can be .png or .jpg)
        
    Returns:
        List of RGB image arrays
    """
    imgs = []
    for img_filename in img_filenames:
        # Try the original filename first
        if os.path.exists(img_filename):
            img = cv2.imread(img_filename)
        else:
            # If original doesn't exist, try switching extension
            if img_filename.endswith('.png'):
                jpg_filename = img_filename.replace('.png', '.jpg')
                if os.path.exists(jpg_filename):
                    img = cv2.imread(jpg_filename)
                else:
                    raise FileNotFoundError(f"Neither {img_filename} nor {jpg_filename} exists")
            elif img_filename.endswith('.jpg'):
                png_filename = img_filename.replace('.jpg', '.png')
                if os.path.exists(png_filename):
                    img = cv2.imread(png_filename)
                else:
                    raise FileNotFoundError(f"Neither {img_filename} nor {png_filename} exists")
            else:
                raise FileNotFoundError(f"Image file {img_filename} not found")
        
        if img is None:
            raise ValueError(f"Failed to load image: {img_filename}")
            
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        imgs.append(rgb)
    return imgs


def compute_box_3d(obj: Object3d, dataset: str = 'kitti') -> np.ndarray:
    """
    Compute 3D bounding box corners from Object3d.
    
    Mathematical Process:
    1. Create 3D box template in object coordinate system
    2. Apply rotation around Y-axis (yaw)
    3. Translate to object center position
    
    Rotation matrix around Y-axis:
    $$\mathbf{R}_y(\theta) = \begin{bmatrix}
    \cos\theta & 0 & \sin\theta \\
    0 & 1 & 0 \\
    -\sin\theta & 0 & \cos\theta
    \end{bmatrix}$$
    
    Args:
        obj: Object3d instance
        dataset: Dataset type (default: 'kitti')
        
    Returns:
        corners_3d: (3, 8) corner coordinates in camera frame
    """
    # 3D bounding box dimensions
    l, w, h = obj.l, obj.w, obj.h
    
    # 3D bounding box corners template (object coordinate system)
    x_corners = [l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2]
    y_corners = [0, 0, 0, 0, -h, -h, -h, -h]
    z_corners = [w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2]
    
    # Rotation matrix around Y-axis (yaw angle)
    R = np.array([
        [np.cos(obj.ry), 0, np.sin(obj.ry)],
        [0, 1, 0],
        [-np.sin(obj.ry), 0, np.cos(obj.ry)]
    ])
    
    # Rotate and translate
    corners_3d = np.vstack([x_corners, y_corners, z_corners])  # (3, 8)
    corners_3d = np.dot(R, corners_3d)
    corners_3d[0, :] = corners_3d[0, :] + obj.t[0]
    corners_3d[1, :] = corners_3d[1, :] + obj.t[1]
    corners_3d[2, :] = corners_3d[2, :] + obj.t[2]
    
    return corners_3d


# Helper functions for calibration and coordinate transformations

def inverse_rigid_trans(Tr):
    """ Inverse a rigid body transform matrix (3x4 as [R|t])
        [R'|-R't; 0|1]
    """
    inv_Tr = np.zeros_like(Tr)  # 3x4
    inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])
    inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])
    return inv_Tr

def read_calib_file(filepath):
    """ Read in a calibration file and parse into a dictionary.
    Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py
    """
    data = {}
    with open(filepath, 'r') as f:
        for line in f.readlines():
            line = line.rstrip()
            if len(line) == 0: continue
            key, value = line.split(':', 1)
            # The only non-float values in these files are dates, which
            # we don't care about anyway
            try:
                data[key] = np.array([float(x) for x in value.split()])
            except ValueError:
                pass
    return data

def cart2hom(pts_3d):
    """ Input: nx3 points in Cartesian
        Oupput: nx4 points in Homogeneous by pending 1
    """
    n = pts_3d.shape[0]
    pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))
    return pts_3d_hom

# Calibration classes (simplified versions for compatibility)

class WaymoCalibration(object):
    """ Calibration matrices and utils
        3d XYZ in <label>.txt are in rect camera coord. 
        2d box xy are in image2 coord
        Points in <lidar>.bin are in Velodyne coord.

        y_image2 = P^2_rect * x_rect
        y_image2 = P^2_rect * R0_rect * Tr_velo_to_cam * x_velo
        x_ref = Tr_velo_to_cam * x_velo
        x_rect = R0_rect * x_ref

        P^2_rect = [f^2_u,  0,      c^2_u,  -f^2_u b^2_x;
                    0,      f^2_v,  c^2_v,  -f^2_v b^2_y;
                    0,      0,      1,      0]
                 = K * [1|t]

        image2 coord:
         ----> x-axis (u)
        |
        |
        v y-axis (v)

        velodyne coord:
        front x, left y, up z

        rect/ref camera coord:
        right x, down y, front z

        Ref (KITTI paper): http://www.cvlibs.net/publications/Geiger2013IJRR.pdf

        TODO(rqi): do matrix multiplication only once for each projection.
    """

    def __init__(self, calib_filepath):
        self.camera_count = 5
        calibs = self.read_calib_file(calib_filepath)
        print(calibs)
        # Projection matrix from rect camera coord to image2 coord
        P_name = ["P"+str(i) for i in range(self.camera_count)] 
        self.P = [np.reshape(calibs[name], [3,4]) for name in P_name] #5 cameras
        
        # Rigid transform from Velodyne coord to reference camera coord
        Tr_velo_to_cam_name = ["Tr_velo_to_cam_"+str(i) for i in range(self.camera_count)] 
        self.V2C = [np.reshape(calibs[name],[3,4]) for name in Tr_velo_to_cam_name] #array for 5 cameras
        
        self.C2V = [self.inverse_rigid_trans(self.V2C[i]) for i in range(self.camera_count)]
        # Rotation from reference camera coord to rect camera coord
        self.R0 = calibs["R0_rect"]  #'R0_rect': array([1., 0., 0., 0., 1., 0., 0., 0., 1.]), not used in Waymo
        self.R0 = np.reshape(self.R0, [3, 3])

    def inverse_rigid_trans(self,Tr):
        """ Inverse a rigid body transform matrix (3x4 as [R|t])
            [R'|-R't; 0|1]
        """
        inv_Tr = np.zeros_like(Tr)  # 3x4
        inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])
        inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])
        return inv_Tr

    def read_calib_file(self, filepath):
        """ Read in a calibration file and parse into a dictionary.
        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py
        """
        data = {}
        with open(filepath, "r") as f:
            for line in f.readlines():
                line = line.rstrip()
                if len(line) == 0:
                    continue
                key, value = line.split(":", 1)
                # The only non-float values in these files are dates, which
                # we don't care about anyway
                try:
                    data[key] = np.array([float(x) for x in value.split()])
                except ValueError:
                    pass

        return data
    
    # convert 3Dbox in rect camera coordinate to velodyne coordinate
    def project_rect_to_velo(self, pts_3d_rect, camera_id=0):
        """ Input: nx3 points in rect camera coord.
            Output: nx3 points in velodyne coord.
        """
        pts_3d_ref = self.project_rect_to_ref(pts_3d_rect)# using R0 to convert to camera rectified coordinate (same to camera coordinate)
        return self.project_ref_to_velo(pts_3d_ref, camera_id)#using C2V
    
    def project_rect_to_ref(self, pts_3d_rect):
        """ Input and Output are nx3 points """
        return np.transpose(np.dot(np.linalg.inv(self.R0), np.transpose(pts_3d_rect)))
    
    def project_ref_to_velo(self, pts_3d_ref, camera_id):
        pts_3d_ref = self.cart2hom(pts_3d_ref)  # nx4
        return np.dot(pts_3d_ref, np.transpose(self.C2V[camera_id]))
    
    def cart2hom(self, pts_3d):
        """ Input: nx3 points in Cartesian
            Oupput: nx4 points in Homogeneous by pending 1
        """
        n = pts_3d.shape[0]
        pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))
        return pts_3d_hom
    
    def project_ref_to_rect(self, pts_3d_ref):
        """ Input and Output are nx3 points """
        return np.transpose(np.dot(self.R0, np.transpose(pts_3d_ref)))
    
    # ===========================
    # ------- 3d to 2d ----------
    # ===========================
    def project_cam3d_to_image(self, pts_3d_rect, cameraid):
        """ Input: nx3 points in rect camera coord.
            Output: nx2 points in image coord.
        """
        pts_3d_rect = self.cart2hom(pts_3d_rect)#nx3 to nx4 by pending 1
        pts_2d = np.dot(pts_3d_rect, np.transpose(self.P[cameraid]))  # nx3
        pts_2d[:, 0] /= pts_2d[:, 2] #normalize the depth pts_2d[:, 2]
        pts_2d[:, 1] /= pts_2d[:, 2]
        return pts_2d[:, 0:2], pts_2d[:, 2] #return points2D and depth
    
    #Liar project to image
    def project_velo_to_cameraid_rect(self, pts_3d_velo, cameraid):
        pts_3d_camid = self.project_velo_to_cameraid(pts_3d_velo, cameraid)
        return self.project_ref_to_rect(pts_3d_camid)#apply R0
    
    def project_velo_to_cameraid(self, pts_3d_velo, cameraid):#project velodyne to camid frame
        pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4
        return np.dot(pts_3d_velo, np.transpose(self.V2C[cameraid]))
    
    def project_velo_to_image(self, pts_3d_velo, cameraid):
        """ Input: nx3 points in velodyne coord.
            Output: nx2 points in image2 coord.
        """
        if pts_3d_velo.shape[1]!=3:
            print("points dimension is not 3")
            pts_3d_velo=pts_3d_velo[:,0:3]
        pts_3d_rect = self.project_velo_to_cameraid_rect(pts_3d_velo, cameraid)
        return self.project_cam3d_to_image(pts_3d_rect, cameraid)


class KittiCalibration(object):
    """ Calibration matrices and utils
        3d XYZ in <label>.txt are in rect camera coord.
        2d box xy are in image2 coord
        Points in <lidar>.bin are in Velodyne coord.

        y_image2 = P^2_rect * x_rect
        y_image2 = P^2_rect * R0_rect * Tr_velo_to_cam * x_velo
        x_ref = Tr_velo_to_cam * x_velo
        x_rect = R0_rect * x_ref

        P^2_rect = [f^2_u,  0,      c^2_u,  -f^2_u b^2_x;
                    0,      f^2_v,  c^2_v,  -f^2_v b^2_y;
                    0,      0,      1,      0]
                 = K * [1|t]

        image2 coord:
         ----> x-axis (u)
        |
        |
        v y-axis (v)

        velodyne coord:
        front x, left y, up z

        rect/ref camera coord:
        right x, down y, front z

        Ref (KITTI paper): http://www.cvlibs.net/publications/Geiger2013IJRR.pdf

        TODO(rqi): do matrix multiplication only once for each projection.
    """

    def __init__(self, calib_filepath, from_video=False):
        if from_video:
            calibs = self.read_calib_from_video(calib_filepath)
        else:
            calibs = self.read_calib_file(calib_filepath)
        # Projection matrix from rect camera coord to image2 coord
        self.P = calibs["P2"]
        self.P = np.reshape(self.P, [3, 4])
        # Rigid transform from Velodyne coord to reference camera coord
        # Handle both standard KITTI format and camera-specific format
        if "Tr_velo_to_cam" in calibs:
            self.V2C = calibs["Tr_velo_to_cam"]
        elif "Tr_velo_to_cam_2" in calibs:
            # Use camera 2 transformation (standard KITTI camera)
            self.V2C = calibs["Tr_velo_to_cam_2"]
        elif "Tr_velo_to_cam_0" in calibs:
            # Fallback to camera 0
            self.V2C = calibs["Tr_velo_to_cam_0"]
        else:
            raise ValueError("No valid Tr_velo_to_cam transformation found in calibration file")
        
        self.V2C = np.reshape(self.V2C, [3, 4])
        self.C2V = self.inverse_rigid_trans(self.V2C)
        # Rotation from reference camera coord to rect camera coord
        self.R0 = calibs["R0_rect"]
        self.R0 = np.reshape(self.R0, [3, 3])

        # Camera intrinsics and extrinsics
        self.c_u = self.P[0, 2]
        self.c_v = self.P[1, 2]
        self.f_u = self.P[0, 0]
        self.f_v = self.P[1, 1]
        self.b_x = self.P[0, 3] / (-self.f_u)  # relative
        self.b_y = self.P[1, 3] / (-self.f_v)

    def read_calib_file(self, filepath):
        """ Read in a calibration file and parse into a dictionary.
        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py
        """
        data = {}
        with open(filepath, "r") as f:
            for line in f.readlines():
                line = line.rstrip()
                if len(line) == 0:
                    continue
                key, value = line.split(":", 1)
                # The only non-float values in these files are dates, which
                # we don't care about anyway
                try:
                    data[key] = np.array([float(x) for x in value.split()])
                except ValueError:
                    pass

        return data

    def read_calib_from_video(self, calib_root_dir):
        """ Read calibration for camera 2 from video calib files.
            there are calib_cam_to_cam and calib_velo_to_cam under the calib_root_dir
        """
        data = {}
        cam2cam = self.read_calib_file(
            os.path.join(calib_root_dir, "calib_cam_to_cam.txt")
        )
        velo2cam = self.read_calib_file(
            os.path.join(calib_root_dir, "calib_velo_to_cam.txt")
        )
        Tr_velo_to_cam = np.zeros((3, 4))
        Tr_velo_to_cam[0:3, 0:3] = np.reshape(velo2cam["R"], [3, 3])
        Tr_velo_to_cam[:, 3] = velo2cam["T"]
        data["Tr_velo_to_cam"] = np.reshape(Tr_velo_to_cam, [12])
        data["R0_rect"] = cam2cam["R_rect_00"]
        data["P2"] = cam2cam["P_rect_02"]
        return data

    def cart2hom(self, pts_3d):
        """ Input: nx3 points in Cartesian
            Oupput: nx4 points in Homogeneous by pending 1
        """
        n = pts_3d.shape[0]
        pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))
        return pts_3d_hom

    def inverse_rigid_trans(self, Tr):
        """ Inverse a rigid body transform matrix (3x4 as [R|t])
            [R'|-R't; 0|1]
        """
        inv_Tr = np.zeros_like(Tr)  # 3x4
        inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])
        inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])
        return inv_Tr

    # ===========================
    # ------- 3d to 3d ----------
    # ===========================
    def project_velo_to_ref(self, pts_3d_velo):
        pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4
        return np.dot(pts_3d_velo, np.transpose(self.V2C))

    def project_ref_to_velo(self, pts_3d_ref):
        pts_3d_ref = self.cart2hom(pts_3d_ref)  # nx4
        return np.dot(pts_3d_ref, np.transpose(self.C2V))

    def project_rect_to_ref(self, pts_3d_rect):
        """ Input and Output are nx3 points """
        return np.transpose(np.dot(np.linalg.inv(self.R0), np.transpose(pts_3d_rect)))

    def project_ref_to_rect(self, pts_3d_ref):
        """ Input and Output are nx3 points """
        return np.transpose(np.dot(self.R0, np.transpose(pts_3d_ref)))

    def project_rect_to_velo(self, pts_3d_rect):
        """ Input: nx3 points in rect camera coord.
            Output: nx3 points in velodyne coord.
        """
        pts_3d_ref = self.project_rect_to_ref(pts_3d_rect)
        return self.project_ref_to_velo(pts_3d_ref)

    def project_velo_to_rect(self, pts_3d_velo):
        pts_3d_ref = self.project_velo_to_ref(pts_3d_velo)
        return self.project_ref_to_rect(pts_3d_ref)

    # ===========================
    # ------- 3d to 2d ----------
    # ===========================
    def project_rect_to_image(self, pts_3d_rect):
        """ Input: nx3 points in rect camera coord.
            Output: nx2 points in image2 coord.
        """
        pts_3d_rect = self.cart2hom(pts_3d_rect)
        pts_2d = np.dot(pts_3d_rect, np.transpose(self.P))  # nx3
        pts_2d[:, 0] /= pts_2d[:, 2]
        pts_2d[:, 1] /= pts_2d[:, 2]
        return pts_2d[:, 0:2]

    def project_velo_to_image(self, pts_3d_velo):
        """ Input: nx3 points in velodyne coord.
            Output: nx2 points in image2 coord.
        """
        pts_3d_rect = self.project_velo_to_rect(pts_3d_velo)
        return self.project_rect_to_image(pts_3d_rect)

    def project_cam3d_to_image(self, pts_3d_cam, cameraid=0):
        """ Input: nx3 points in camera coord.
            Output: nx2 points in image coord.
        """
        return self.project_rect_to_image(pts_3d_cam)

    def project_velo_to_cameraid(self, pts_3d_velo, cameraid=0):
        """ Input: nx3 points in velodyne coord.
            Output: nx3 points in camera coord.
        """
        return self.project_velo_to_rect(pts_3d_velo)


def datasetinfo(datasetname: str) -> Tuple[int, int]:
    """
    Get dataset-specific camera configuration.
    
    Args:
        datasetname: 'waymokitti' or 'kitti'
        
    Returns:
        Tuple of (camera_index, max_camera_count)
    """
    if datasetname.lower() == 'waymokitti':
        camera_index = 0  # front camera of Waymo is image_0
        max_cameracount = 5
    elif datasetname.lower() == 'kitti':
        camera_index = 2  # front camera of KITTI is image_2
        max_cameracount = 5
    return camera_index, max_cameracount


def getcalibration(datasetname: str, calibration_file: str):
    """
    Get appropriate calibration object for dataset.
    
    Args:
        datasetname: 'waymokitti' or 'kitti'
        calibration_file: Path to calibration file
        
    Returns:
        Calibration object
    """
    if datasetname.lower() == 'waymokitti':
        calib = WaymoCalibration(calibration_file)
    elif datasetname.lower() == 'kitti':
        calib = KittiCalibration(calibration_file)
    return calib


def visualize_lidar_with_boxes_open3d(pc_velo: np.ndarray, 
                                     object3dlabels: List[Object3d],
                                     calib,
                                     point_cloud_range: List[float],
                                     color_by: str = 'intensity',
                                     save_path: Optional[str] = None,
                                     headless: bool = False) -> None:
    """
    Visualize LiDAR point cloud with 3D bounding boxes using Open3D.
    
    This function replaces the original Mayavi-based visualization.
    
    Args:
        pc_velo: Point cloud array (N, 4) or (N, 6)
        object3dlabels: List of Object3d instances
        calib: Calibration object
        point_cloud_range: Point filtering range
        color_by: Point cloud coloring method
        save_path: Optional path to save PLY file
        headless: Whether to run in headless mode
    """
    print("Visualizing with Open3D...")
    
    # Create visualizer
    visualizer = Open3DVisualizer(headless=headless)
    
    # Convert Object3d labels to format expected by visualizer
    boxes_for_viz = []
    for obj in object3dlabels:
        if obj.type != "DontCare":
            boxes_for_viz.append(obj)
    
    # Visualize scene with calibration for coordinate transformation
    visualizer.visualize_scene(
        points=pc_velo,
        boxes=boxes_for_viz,
        point_cloud_range=point_cloud_range,
        color_by=color_by,
        save_path=save_path,
        show_coordinate_frame=True,
        show_ground_grid=True,
        calib=calib  # Pass calibration for coordinate transformation
    )


# Image visualization functions (maintained for compatibility)

def plt_multiimages(images: List[np.ndarray], 
                   objectlabels: List[List[Object3d]], 
                   datasetname: str, 
                   order: int = 1,
                   save_path: Optional[str] = None) -> None:
    """Display multiple images with 2D bounding boxes."""
    plt.figure(order, figsize=(16, 9))
    camera_count = len(images)
    
    # Debug: Print information about labels
    print(f"Debug: Processing {camera_count} cameras")
    for i, labels in enumerate(objectlabels):
        print(f"Camera {i}: {len(labels)} labels")
        for j, obj in enumerate(labels[:3]):  # Print first 3 objects
            print(f"  Object {j}: type={obj.type}, box2d={obj.box2d}")
    
    for count in range(camera_count):
        if datasetname.lower() == 'waymokitti':
            index = waymocameraorder[count]
            pltshow_image_with_boxes(index, images[index], objectlabels[index], [3, 3, count+1])
        elif datasetname.lower() == 'kitti':
            index = count
            pltshow_image_with_boxes(index, images[index], objectlabels[index], [1, 2, count+1])
    
    # Save the figure if save_path is provided
    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"2D visualization saved to: {save_path}")
    
    plt.show()


def pltshow_image_with_boxes(cameraid: int, img: np.ndarray, 
                           objects: List[Object3d], layout: List[int], 
                           cmap=None) -> None:
    """Show image with 2D bounding boxes."""
    ax = plt.subplot(*layout)
    img1 = np.copy(img)
    plt.imshow(img1, cmap=cmap)
    plt.title(cameraname_map[cameraid])
    
    if not objects or len(objects) == 0:
        return
    
    valid_boxes_count = 0
    for obj in objects:
        if obj.type == "DontCare":
            continue
        
        box = obj.box2d
        objectclass = obj.type
        
        if objectclass in INSTANCE_Color.keys():
            colorlabel = INSTANCE_Color[objectclass]
            xmin, ymin, xmax, ymax = box
            width = xmax - xmin
            height = ymax - ymin
            
            # Check if bounding box is valid (not all zeros)
            if (xmin == 0 and ymin == 0 and xmax == 0 and ymax == 0):
                continue  # Skip invalid bounding boxes
            
            if height > 0 and width > 0:
                valid_boxes_count += 1
                ax.add_patch(patches.Rectangle(
                    xy=(xmin, ymin),
                    width=width,
                    height=height,
                    linewidth=1,
                    edgecolor=colorlabel,
                    facecolor='none'
                ))
                ax.text(xmin, ymin, objectclass, color=colorlabel, fontsize=8)
        else:
            print(f"Object not in KITTI: {objectclass}")
    
    # Add warning text if no valid boxes found
    if valid_boxes_count == 0 and len(objects) > 0:
        ax.text(10, 30, "No valid 2D bounding boxes\n(all coordinates are zero)", 
                color='yellow', fontsize=10, weight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='red', alpha=0.7))
    
    plt.grid(False)
    plt.axis('on')


def main():
    """
    Main function for command-line interface.
    
    Provides the same interface as the original waymokittiall.py but uses Open3D
    for 3D visualization instead of Mayavi.
    """
    parser = argparse.ArgumentParser(description="KITTI/Waymo2KITTI visualization with Open3D")
    parser.add_argument(
        "--root_path", 
        default='/data/Datasets/waymo143/waymokitti/',
        help="Root folder path"
    )
    parser.add_argument("--index", default="000000", help="File index")
    parser.add_argument("--dataset", default="waymokitti", help="Dataset name (kitti/waymokitti)")
    parser.add_argument("--camera_count", default=None, type=int, help="Number of cameras used (default: all cameras for dataset)")
    parser.add_argument("--headless", action="store_true", help="Run in headless mode")
    parser.add_argument("--output_path", default="outputs/", help="Output directory for saving visualizations")
    parser.add_argument("--save_ply", help="Path to save PLY file")
    parser.add_argument("--point_cloud_range", nargs=6, type=float,
                       default=[-100, -60, -8, 100, 60, 8],
                       help="Point cloud range [xmin ymin zmin xmax ymax zmax]")
    parser.add_argument("--n_vec", default=6, type=int, 
                       help="Number of values per point (4 for KITTI, 6 for Waymo2KITTI)")
    parser.add_argument("--color_by", default="intensity", 
                       choices=["intensity", "elongation", "mask_indices", "height", "none"],
                       help="Point cloud coloring method")
    
    args = parser.parse_args()
    
    basedir = args.root_path
    idx = int(args.index)
    
    camera_index, max_cameracount = datasetinfo(args.dataset)
    
    # Set default camera_count based on dataset
    if args.camera_count is None:
        if args.dataset.lower() == 'waymokitti':
            camera_count = max_cameracount  # Use all 5 cameras for Waymo
        else:
            camera_count = 1  # Use single camera for KITTI
    else:
        camera_count = min(args.camera_count, max_cameracount)  # Limit to max available
    
    # File paths
    filename = f"{idx:06d}.png"
    
    # Construct image file paths based on dataset type
    if args.dataset.lower() == 'waymokitti':
        # Waymo has cameras 0-4 (image_0, image_1, image_2, image_3, image_4)
        image_files = [os.path.join(basedir, f"image_{i}", filename) for i in range(camera_count)]
        
        # Check if we have individual label directories (label_0, label_1, etc.) or KITTI-style (label_2)
        label_0_dir = os.path.join(basedir, "label_0")
        label_2_dir = os.path.join(basedir, "label_2")
        
        if os.path.exists(label_0_dir):
            # Waymo-style: separate label directory for each camera
            labels_files = [os.path.join(basedir, f"label_{i}", filename.replace('png', 'txt')) for i in range(camera_count)]
        elif os.path.exists(label_2_dir):
            # KITTI-style: use label_2 for all cameras (dummy data format)
            labels_files = [os.path.join(basedir, "label_2", filename.replace('png', 'txt')) for i in range(camera_count)]
        else:
            # Fallback to original logic
            labels_files = [os.path.join(basedir, f"label_{i}", filename.replace('png', 'txt')) for i in range(camera_count)]
    else:
        # KITTI has cameras starting from camera_index (usually 2 for image_2)
        image_files = [os.path.join(basedir, f"image_{i+camera_index}", filename) for i in range(camera_count)]
        labels_files = [os.path.join(basedir, f"label_{camera_index}", filename.replace('png', 'txt'))]
    
    calibration_file = os.path.join(basedir, 'calib', filename.replace('png', 'txt'))
    label_all_file = os.path.join(basedir, 'label_all', filename.replace('png', 'txt'))
    lidar_filename = os.path.join(basedir, 'velodyne', filename.replace('png', 'bin'))
    
    print(f"Loading {camera_count} camera images:")
    for i, img_file in enumerate(image_files):
        print(f"  Camera {i}: {img_file}")
    
    # Load data
    print(f"Loading data for index {idx}...")
    
    # Load LiDAR points
    pc_velo = load_velo_scan(
        lidar_filename, 
        dtype=np.float32, 
        n_vec=args.n_vec,  # Use command line argument
        filterpoints=True, 
        point_cloud_range=args.point_cloud_range
    )
    
    # Load calibration
    calib = getcalibration(args.dataset, calibration_file)
    
    # Load images and labels
    images = load_image(image_files)
    objectlabels = read_multi_label(labels_files)
    
    # Display 2D visualizations
    if not args.headless:
        # Create save path for 2D visualization
        save_2d_path = None
        if args.output_path:
            save_2d_path = os.path.join(args.output_path, f"{filename.replace('.png', '')}_2d_visualization.png")
        
        plt_multiimages(images, objectlabels, args.dataset, save_path=save_2d_path)
        plt.show()
    
    # Load 3D labels
    if args.dataset.lower() == 'waymokitti':
        object3dlabels = read_label(label_all_file)
    elif args.dataset.lower() == 'kitti':
        object3dlabels = objectlabels[0]
    
    # 3D visualization with Open3D
    save_path = args.save_ply if args.save_ply else None
    if args.output_path and not save_path:
        os.makedirs(args.output_path, exist_ok=True)
        save_path = os.path.join(args.output_path, f"{filename.replace('.png', '')}_3d_visualization.ply")
    elif args.headless and not save_path:
        save_path = f"scene_{idx:06d}.ply"
    
    visualize_lidar_with_boxes_open3d(
        pc_velo=pc_velo,
        object3dlabels=object3dlabels,
        calib=calib,
        point_cloud_range=args.point_cloud_range,
        color_by=args.color_by,
        save_path=save_path,
        headless=args.headless
    )
    
    print("Visualization complete!")


if __name__ == "__main__":
    main()